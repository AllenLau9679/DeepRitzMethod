\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

\usepackage[preprint]{nips_2018}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{palatino}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\title{A Brief Report of Deep Ritz Method}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Zeyu Jia\\School of Mathematical Science\\Peking University\\ \texttt{1600010603} \\
  \And
  Dinghuai Zhang\\School of Mathematical Science\\Peking University
  \And
  Zhengming Zou\\School of Mathematical Science\\Peking University
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}


\section{The Ritz Method}


\section{Deep Ritz Method with neural networks}
\par Using the Ritz Method mentioned above, it is natural to think of solving pde with deep neural networks. Considering the pde 
\begin{equation}
\Delta u(x)=f(x),x\in \Omega
\end {equation}
\par According to Ritz Method, what we need to do is
\begin{equation}
\min\limits_{u\in H}{I(u)},
\end{equation}
where
\begin{equation} 
I(u)=\int_\Omega(\frac{1}{2}|\nabla u(x)|^2-f(x)u(x))dx,
\end{equation}
and $H$ is the set of admissible function. Our main idea is to facilitate the multi-layer neural network approximation function $u(x)$ and use the gradient descent algorithm to get the final result.\\

\subsection{Building trail function}
\par We mainly use a nonlinear transformation $x \to u_{\theta}(x)$ defined by deep neural networks to approximate function $u(x)$. Here $\theta$ denotes all parameters in our model. Similar to ResNet structure, we use several blocks to construct our networks, each block consists of two linear transformation, two activation functions and a residual connection. The $i$-th block can be expressed as 
\begin{equation}\label{res_equ}
s_i=\phi(W_{i2}\phi(W_{i1}s_{i_1}+b_{i1})+b_{i2})+s_{i-1}.
\end{equation}
\thispagestyle{empty}
% 流程图定义基本形状
	\begin{figure}
	\caption{Network Structure of Deep Ritz Method}
	\centering
	\tikzstyle{startstop} = [rectangle, rounded corners, minimum width = 2cm, minimum height=1cm,text centered, draw = black, fill=red]
	\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=purple]
	\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=0.5cm, text centered, draw=black, fill=pink]
	\tikzstyle{decision} = [diamond, aspect = 3, text centered, draw=black, fill=blue]
	% 箭头形式
	\tikzstyle{arrow} = [->,>=stealth]
	\begin{tikzpicture}[node distance=1cm]
	%定义流程图具体形状
	\node[io,  yshift = -1cm](in1){Input};
	\node[process, below of = in1, yshift = -0.5cm](fc1){FC layer(size m) + activation function};
	\node[process, below of = fc1, yshift = -0.5cm](fc2){FC layer(size m) + activation function};
	\node[decision, right  of = fc1, xshift = 5cm, yshift= -1cm](residual 1){Decision 1 ?};
	\node[process, below of = fc2 , yshift = -0.5cm](fc3){FC layer(size m) + activation function};
	\node[process, below of = fc3, yshift = -0.5cm](fc4){FC layer(size m) + activation function};
	\node[decision, right  of = fc3, xshift = 5cm, yshift= -1cm](residual 2){Decision 1 ?};
	\node[process, below of = fc4, yshift = -0.5cm](fc5){FC layer(size 1) };
	\node[io, below of = fc5, yshift = -0.5cm](out1){Output};
	\coordinate (point1) at (-3cm, -6cm);
	%连接具体形状
	\draw [arrow] (in1) -- (fc1);
	\draw [arrow] (in1) -| node  [right] {} (residual 1);
	\draw [arrow] (fc1) -- (fc2);
	\draw [arrow] (fc2) -- (fc3);
	\draw [arrow] (fc3) -- (fc4);
	\draw [arrow] (fc4) -- (fc5);
	%\draw (residual 1) -- node [above] {Y} (point1);
	\draw [arrow] (residual 1) |- node [right] {} (fc3);
	\draw [arrow] (fc3) -- (out1);
	\end{tikzpicture}
\end{figure}

where $s_{i}$ is the output of the $i$-th layer. $W_{i1},W_{i2}\in R^{m\times m},b_{i1},b_{i2}\in R^{m}$ are defined by linear transformation. $\phi$ is the activation function.

\par Because our pde involves the Laplace transform, we naturally hope that the second derivative of the function $u(x)$ is not a constant. So we need to pick a proper activation function to ensure the networks' nonlinearity. In our model, we have decided to use 
\begin{equation}
\phi(x)=\max\{x^3,0\}
\end{equation}

\par The residual connection in \ref{res_equ} %这里需要一个跳转而且不一定是(4)% 
helps to avoid the gradient vanishing problems.
After several blocks, we use a linear transform to get the final result. The whole network can be expressed as
\begin{equation}
u_{\theta}(x)=a\cdot f_n(x) \circ ...\circ f_1(x)+b
\end{equation}
$f_i(x)$ is the $i$-th block. $a\in R^m,b\in R$ is defined by the final linear transform. Note that the input vector $x$ is not necessarily m-dimensional. In order to handle this mismatch, we can pad $x$ by a zero vector. In our model, we always assume $d<m$.\\
After building our trail function, we are left to minimize the $I(u)$ in (3)%这里需要一个跳转而且不一定是(3)% 
\subsection{Euler numerical integration method}
The first problem we need to solve is to calculate the integral in (3)%这里需要一个跳转而且不一定是(3)% 
 . For simplicity, define:
 \begin{equation}
 g(x,\theta)=|\nabla u(x)|^2-f(x)u(x)
 \end{equation}
 then the $I(u)$ can be expressed as
 \begin{equation}
 I(u)=\int _{\Omega}g(x,\theta)dx
 \end{equation}
 Obviously, it's impossible to calculate this integral directly. We use Euler numerical integration method to approximate the integral.
 \begin{equation}
 I(u)=L(x,\theta)=\frac{1}{N}\sum\limits_{i=1}^{N}g(x_i,\theta)
 \end{equation}
 Where each $x_i$ corresponds to a data point. Each data point is taken from the grid $[-1,1]\times [-1,1]$ in steps of 0.001.
 \subsection{The stochastic gradient descent algorithm}
 In deep learning, stochastic gradient descent (SGD) is a common method to minimize the loss function. In this problem, we also choose SGD method to minimize $I(u)$, which can be expressed as:
 \begin{equation}
 \theta^{k+1}=\theta^{k}-\eta \nabla_{\theta}\frac{1}{N}\sum\limits_{i=1}^{N}g(x_{i,k},\theta^k)
 \end{equation}
 where $k$ is the number of iterations.$\{x_{i,k}\}$ is the randomly selected data from the grid. In SGD, we only select a small number of data points from the grid at a time. Due to our limited computing capiticy, we should make a compromise  between computing the true gradient and the gradient at a single example. So we choose SGD to compute the gradient against more than one training example (also called a "mini-batch") at each step. In order to optimize our training process, we use the Adam version of SGD.


\section{Our improvements}
Along with the footsteps of professor E, we have achieved very good results. Based on the results already available, we want to make further improvements. 

\subsection{L2 regularization}

\subsection{Self adaptive}


\section{Numerical Results}

\subsection{The Poisson Equation}
\par Considering the Poisson equation:
\begin{equation}
\left\{
\begin{aligned}
 \Delta u=1,& x\in \Omega \\
 u=0, &x\in \partial \Omega \\
 \end{aligned}
\right.
\end{equation}

Here $\Omega =\{(x,y)| x^2+y^2<1\}$.
\par The exact solution to this problem is 
\begin{equation}
u=\frac{1}{4}(x^2+y^2-1)
\end{equation}

\par As described above, we use three blocks(six fully connected layers ) and a final linear transform with $m=10$ to build our networks. There is a total of 671 parameters in our model. Considering the boundary condition, we need to make some modifications to our model. We have decided to use a penalty method and the modified function is:
\begin{equation}
I(u)=\int_{\Omega}(\frac{1}{2}|\nabla u(x)|^2-u(x))dx+\gamma\int_{\Omega}|\Delta u(x)|^2dx+\beta\int_{\partial \Omega}u(x)^2dx
\end{equation}



\end{document}