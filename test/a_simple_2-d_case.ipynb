{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a simple case\n",
    "Consider the following Possion Equation\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\Delta u = 1\\qquad &u\\in\\Omega\\\\\n",
    "    u = 0\\qquad &u\\in\\partial\\Omega.\n",
    "\\end{cases}$$\n",
    "Here $\\Omega = \\{(x, y)|x^2+y^2 < 1\\}$\n",
    "\n",
    "The exact solution to this problem is $$u = \\frac{1}{4}(x^2+y^2-1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "m = 10\n",
    "learning_rate = 1\n",
    "iterations = 8000  #default 10000\n",
    "print_every_iter = 100\n",
    "beta = 10 #coefficient for the regularization term in the loss expression, is set to be 1000 in section 3.1\n",
    "n1 = 1000 #number of points in (0,1)^m\n",
    "n2 = 100  #number of points on the border of (0,1)^m\n",
    "n3 = 100  #number of points used for evaluating the error\n",
    "\n",
    "class DeepRitzNet(torch.nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super(DeepRitzNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(m,m)\n",
    "        self.linear2 = torch.nn.Linear(m,m)\n",
    "        self.linear3 = torch.nn.Linear(m,m)\n",
    "        self.linear4 = torch.nn.Linear(m,m)\n",
    "        self.linear5 = torch.nn.Linear(m,m)\n",
    "        self.linear6 = torch.nn.Linear(m,m)\n",
    "        \n",
    "        self.linear7 = torch.nn.Linear(m,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = (F.relu(self.linear1(x))) ** 3\n",
    "        y = (F.relu(self.linear2(x))) ** 3\n",
    "        y += x\n",
    "        x = y\n",
    "        y = (F.relu(self.linear3(x))) ** 3\n",
    "        y = (F.relu(self.linear4(x))) ** 3\n",
    "        y += x\n",
    "        x = y\n",
    "        y = (F.relu(self.linear5(x))) ** 3\n",
    "        y = (F.relu(self.linear6(x))) ** 3\n",
    "        y += x\n",
    "        y = (F.relu(self.linear7(x))) ** 3\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_graph():\n",
    "    points = np.arange(-1, 1, 0.1)\n",
    "    xs, ys = np.meshgrid(points, points)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xl, yl = xs.size()\n",
    "    z = np.zeros((xl, yl))\n",
    "    for i in range(xl):\n",
    "        for j in range(yl):      \n",
    "            re = np.zeros(m)\n",
    "            re[0] = xs[i, j]\n",
    "            re[1] = ys[i, j]\n",
    "            re = torch.tensor(re)        \n",
    "            z[i, j] = model(re.float()).item() - U_groundtruth(re)\n",
    "    \n",
    "    plt.imshow(z, cmap=cm.hot)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#U_groundtruth = 1/4*(x^2+y^2)-1/4\n",
    "#take in a (m,) tensor (x, y, ...)\n",
    "def U_groundtruth(t):\n",
    "    re = (t[0] ** 2 + t[1] ** 2 - 1).item() / 4\n",
    "    return re\n",
    "\n",
    "#turn a (2,) tensor/ndarray to a (m,) tensor\n",
    "def zeropad(x_2, m):\n",
    "    x_10 = torch.zeros(m, )\n",
    "    x_10[0] = x_2[0]\n",
    "    x_10[1] = x_2[1]\n",
    "    return x_10\n",
    "    \n",
    "#sample a (m,) tensor on the border of the unit circle\n",
    "def on_sample(m):\n",
    "    theta = np.random.rand() * 2 * pi\n",
    "    re = np.zeros(m)\n",
    "    re[0] = math.cos(theta)\n",
    "    re[1] = math.sin(theta)\n",
    "    re = torch.tensor(re, requires_grad=True)\n",
    "    return re\n",
    "\n",
    "#sample a (m,) tensor in the unit circle\n",
    "def in_sample(m):\n",
    "    r = sqrt(np.random.rand())\n",
    "    theta = np.random.rand() * 2 * pi\n",
    "    re = np.zeros(m)\n",
    "    re[0] = r * math.cos(theta)\n",
    "    re[1] = r * math.sin(theta)\n",
    "    re = torch.tensor(re, requires_grad=True)\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = DeepRitzNet(m)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "in_error_iter = [] #record the error in Omega every print_every_iter=100 times\n",
    "on_error_iter = [] #record the error on the border of Omega every print_every_iter=100 times\n",
    "\n",
    "for i in range(iterations):\n",
    "    #calculate the loss \n",
    "    loss = torch.zeros(1)\n",
    "    for t in range(n1):\n",
    "        #if I miss out the \".float()\" there will be an error and I don't know why\n",
    "        #It seems to have something to do with the usage of relu()**3 in DeepRitzNet\n",
    "        x_input = in_sample(m)\n",
    "        y = model(x_input.float())\n",
    "        #there will be an error without \"retain_graph=True\" , I don't know why\n",
    "        y.backward(retain_graph = True)\n",
    "        \n",
    "        loss += 0.5 * ((x_input.grad.float()[0]) ** 2 + (x_input.grad.float()[1]) ** 2) - y\n",
    "    loss /= n1\n",
    "    print(loss)\n",
    "    \n",
    "    regularization = torch.zeros(1)\n",
    "    for t in range(n2):\n",
    "        x_input = on_sample(m).float()\n",
    "        y = model(x_input)\n",
    "        regularization += y ** 2   \n",
    "    regularization *= beta / n2\n",
    "    \n",
    "    print(regularization)\n",
    "    \n",
    "    loss += regularization\n",
    "    \n",
    "    draw_graph()\n",
    "    \n",
    "    #and step the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    draw_graph()\n",
    "    \n",
    "        \n",
    "print(\"Traning Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print the error\n",
    "    if((i+1) % print_every_iter == 0):\n",
    "        in_error = 0\n",
    "        on_error = 0\n",
    "        \n",
    "        for t in range(n3):\n",
    "            in_x_test = in_sample(m)\n",
    "            in_error_instant = abs((model(in_x_test.float()) -\n",
    "                                    U_groundtruth(in_x_test.float())).item())\n",
    "            in_error = max(in_error, in_error_instant)\n",
    "            \n",
    "            on_x_test = on_sample(m)\n",
    "            on_error_instant = abs((model(on_x_test.float()) -\n",
    "                                    U_groundtruth(on_x_test.float())).item())\n",
    "            on_error = max(on_error,on_error_instant)\n",
    "            \n",
    "        in_error_iter.append(in_error)\n",
    "        on_error_iter.append(on_error)\n",
    "        \n",
    "        print(\"Error in Omega at the\",i+1,\"th iteration:\",in_error)\n",
    "        print(\"Error on the border of Omega at the\",i+1,\"th iteration:\",on_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    #calculate the loss \n",
    "    loss=torch.zeros(1)\n",
    "    for t in range(n1):\n",
    "        #if I miss out the \".float()\" there will be an error and I don't know why\n",
    "        #It seems to have something to do with the usage of relu()**3 in DeepRitzNet\n",
    "        x_input=in_sample()\n",
    "        y=model(x_input.float())\n",
    "        #there will be an error without \"retain_graph=True\" , I don't know why\n",
    "        y.backward(retain_graph=True)\n",
    "        loss+=0.5*((x_input.grad.float()[0])**2+(x_input.grad.float()[1])**2)-y\n",
    "    loss/=n1\n",
    "    \n",
    "    regularization=torch.zeros(1)\n",
    "    for t in range(n2):\n",
    "        x_input=on_sample().float()\n",
    "        y=model(x_input)\n",
    "        regularization+=y**2   \n",
    "    regularization*=beta/n2\n",
    "    \n",
    "    loss+=regularization\n",
    "    \n",
    "    #and step the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the error\n",
    "    if((i+1)%print_every_iter==0):\n",
    "        in_error=0\n",
    "        on_error=0\n",
    "        \n",
    "        for t in range(n3):\n",
    "            in_x_test=in_sample()\n",
    "            in_error_instant=abs((model(in_x_test.float())-U_groundtruth(in_x_test.float())).item())\n",
    "            in_error=max(in_error,in_error_instant)\n",
    "            \n",
    "            on_x_test=on_sample()\n",
    "            on_error_instant=abs((model(on_x_test.float())-U_groundtruth(on_x_test.float())).item())\n",
    "            on_error=max(on_error,on_error_instant)\n",
    "            \n",
    "        in_error_iter.append(in_error)\n",
    "        on_error_iter.append(on_error)\n",
    "        \n",
    "        print(\"Loss at the\",i+1,\"th iteration:\", loss)\n",
    "        print(\"Error in Omega at the\",i+1,\"th iteration:\",in_error)\n",
    "        print(\"Error on the border of Omega at the\",i+1,\"th iteration:\",on_error)\n",
    "        \n",
    "print(\"Traning Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pytorch\n",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
