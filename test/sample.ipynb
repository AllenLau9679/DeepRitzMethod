{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.019859048, 125.23586]\n",
      "100 [0.32488427, 0.36665419]\n",
      "200 [0.3191444, 0.32759154]\n",
      "300 [0.32372731, 0.32992133]\n",
      "400 [0.3358303, 0.3400034]\n",
      "500 [0.335677, 0.33898401]\n",
      "600 [0.33066356, 0.33449349]\n",
      "700 [0.31779847, 0.32097775]\n",
      "800 [0.32656446, 0.33022487]\n",
      "900 [0.33012098, 0.33388269]\n",
      "1000 [0.32620019, 0.32952008]\n",
      "1100 [0.32562566, 0.3281208]\n",
      "1200 [0.31879967, 0.32574421]\n",
      "1300 [0.32423192, 0.32734352]\n",
      "1400 [0.32746983, 0.33219719]\n",
      "1500 [0.33299598, 0.34284312]\n",
      "1600 [0.33669627, 0.33912921]\n",
      "1700 [0.33634764, 0.33911106]\n",
      "1800 [0.31584153, 0.32216221]\n",
      "1900 [0.3362205, 0.34301126]\n",
      "2000 [0.33113018, 0.33414567]\n",
      "2100 [0.33061963, 0.33525771]\n",
      "2200 [0.35651416, 0.35867777]\n",
      "2300 [0.32503662, 0.33176523]\n",
      "2400 [0.33971858, 0.35044846]\n",
      "2500 [0.32888752, 0.33359167]\n",
      "2600 [0.3337222, 0.34100449]\n",
      "2700 [0.33345425, 0.33842975]\n",
      "2800 [0.32667151, 0.33223796]\n",
      "2900 [0.33534667, 0.33896041]\n",
      "3000 [0.33187094, 0.3354418]\n",
      "3100 [0.33362794, 0.33781081]\n",
      "3200 [0.32812005, 0.33089873]\n",
      "3300 [0.32025704, 0.33429351]\n",
      "3400 [0.33412987, 0.33626366]\n",
      "3500 [0.3276988, 0.33374739]\n",
      "3600 [0.3337321, 0.33872163]\n",
      "3700 [0.32226604, 0.33169043]\n",
      "3800 [0.32215032, 0.33409914]\n",
      "3900 [0.333368, 0.33710378]\n",
      "4000 [0.33319509, 0.33842242]\n",
      "4100 [0.3260116, 0.33228442]\n",
      "4200 [0.32633671, 0.33265197]\n",
      "4300 [0.32525304, 0.32826769]\n",
      "4400 [0.33243847, 0.34072706]\n",
      "4500 [0.31503806, 0.3276675]\n",
      "4600 [0.32179207, 0.3252078]\n",
      "4700 [0.32765046, 0.33160341]\n",
      "4800 [0.31869432, 0.32210222]\n",
      "4900 [0.35253948, 0.36272281]\n",
      "5000 [0.33063158, 0.34034887]\n",
      "5100 [0.31697053, 0.32290116]\n",
      "5200 [0.34810975, 0.35548359]\n",
      "5300 [0.32782191, 0.33518359]\n",
      "5400 [0.3303493, 0.33564317]\n",
      "5500 [0.3328529, 0.34101155]\n",
      "5600 [0.32892895, 0.33429897]\n",
      "5700 [0.34323934, 0.34620118]\n",
      "5800 [0.32886341, 0.33300233]\n",
      "5900 [0.33004493, 0.34927964]\n",
      "6000 [0.32567441, 0.32945448]\n",
      "6100 [0.36137548, 0.36677223]\n",
      "6200 [0.33583355, 0.3376385]\n",
      "6300 [0.33013856, 0.33342794]\n",
      "6400 [0.33401611, 0.33938885]\n",
      "6500 [0.3228685, 0.33085218]\n",
      "6600 [0.32881761, 0.33587152]\n",
      "6700 [0.32925919, 0.33238143]\n",
      "6800 [0.32976249, 0.33141458]\n",
      "6900 [0.33600914, 0.33945018]\n",
      "7000 [0.32209975, 0.32700315]\n",
      "7100 [0.34235582, 0.3454119]\n",
      "7200 [0.32782048, 0.33449522]\n",
      "7300 [0.32795683, 0.33151999]\n",
      "7400 [0.32172325, 0.33106738]\n",
      "7500 [0.32021192, 0.32270238]\n",
      "7600 [0.33497748, 0.33664629]\n",
      "7700 [0.32823852, 0.32990596]\n",
      "7800 [0.33234254, 0.33689004]\n",
      "7900 [0.32545525, 0.32942146]\n",
      "8000 [0.33158597, 0.33577687]\n",
      "8100 [0.31952399, 0.32309446]\n",
      "8200 [0.33375981, 0.33641663]\n",
      "8300 [0.34459618, 0.34755516]\n",
      "8400 [0.32931203, 0.34354812]\n",
      "8500 [0.32708097, 0.34157091]\n",
      "8600 [0.32594904, 0.32972825]\n",
      "8700 [0.32943305, 0.36932513]\n",
      "8800 [0.33497229, 0.33765507]\n",
      "8900 [0.32955679, 0.34125441]\n",
      "9000 [0.34061864, 0.34637973]\n",
      "9100 [0.3259345, 0.33213139]\n",
      "9200 [0.3148987, 0.31857201]\n",
      "9300 [0.32860696, 0.33098993]\n",
      "9400 [0.34506744, 0.35945433]\n",
      "9500 [0.32736281, 0.32905096]\n",
      "9600 [0.32012299, 0.32143116]\n",
      "9700 [0.32563859, 0.32983726]\n",
      "9800 [0.31667727, 0.31899083]\n",
      "9900 [0.3382318, 0.34590644]\n",
      "10000 [0.31947079, 0.32269746]\n",
      "10100 [0.33617532, 0.33858308]\n",
      "10200 [0.32994223, 0.33238044]\n",
      "10300 [0.32765755, 0.32999641]\n",
      "10400 [0.32652208, 0.3281585]\n",
      "10500 [0.32985798, 0.33181751]\n",
      "10600 [0.3405526, 0.3425864]\n",
      "10700 [0.34296674, 0.34564823]\n",
      "10800 [0.32472667, 0.32792029]\n",
      "10900 [0.33110297, 0.33372959]\n",
      "11000 [0.32527122, 0.3277427]\n",
      "11100 [0.32071474, 0.32435676]\n",
      "11200 [0.317312, 0.3187595]\n",
      "11300 [0.32136661, 0.32397413]\n",
      "11400 [0.32482606, 0.32836953]\n",
      "11500 [0.34471494, 0.34823778]\n",
      "11600 [0.31560975, 0.31739724]\n",
      "11700 [0.30996802, 0.31196082]\n",
      "11800 [0.31164119, 0.31400439]\n",
      "11900 [0.33394918, 0.33686233]\n",
      "12000 [0.34355834, 0.34728897]\n",
      "12100 [0.32021824, 0.3241998]\n",
      "12200 [0.34696764, 0.35328484]\n",
      "12300 [0.33042261, 0.33218306]\n",
      "12400 [0.33169895, 0.33264911]\n",
      "12500 [0.34334961, 0.34689659]\n",
      "12600 [0.32412255, 0.32678351]\n",
      "12700 [0.31910062, 0.32124987]\n",
      "12800 [0.32435727, 0.32826695]\n",
      "12900 [0.3320083, 0.33424762]\n",
      "13000 [0.3342213, 0.34015894]\n",
      "13100 [0.337495, 0.34210718]\n",
      "13200 [0.32254297, 0.32592261]\n",
      "13300 [0.32789531, 0.33124712]\n",
      "13400 [0.33130431, 0.33362034]\n",
      "13500 [0.33073175, 0.33394867]\n",
      "13600 [0.32906303, 0.3406947]\n",
      "13700 [0.32378051, 0.32662269]\n",
      "13800 [0.33780798, 0.34259468]\n",
      "13900 [0.32623711, 0.32986721]\n",
      "14000 [0.33530414, 0.3375383]\n",
      "14100 [0.32591918, 0.32993644]\n",
      "14200 [0.3386609, 0.34277448]\n",
      "14300 [0.32025105, 0.32182881]\n",
      "14400 [0.33608955, 0.33907235]\n",
      "14500 [0.34479824, 0.34721103]\n",
      "14600 [0.3309789, 0.33330816]\n",
      "14700 [0.33848199, 0.34049094]\n",
      "14800 [0.32667586, 0.33033389]\n",
      "14900 [0.32035697, 0.32321614]\n",
      "15000 [0.3227962, 0.32637328]\n",
      "15100 [0.31711933, 0.31920487]\n",
      "15200 [0.32401067, 0.32556379]\n",
      "15300 [0.32246312, 0.32341704]\n",
      "15400 [0.33383477, 0.33523241]\n",
      "15500 [0.31784302, 0.32102731]\n",
      "15600 [0.33044451, 0.33268049]\n",
      "15700 [0.33867475, 0.34284291]\n",
      "15800 [0.33080888, 0.33344644]\n",
      "15900 [0.31882581, 0.32273635]\n",
      "16000 [0.31964123, 0.32239869]\n",
      "16100 [0.3314887, 0.33499652]\n",
      "16200 [0.32261333, 0.32520595]\n",
      "16300 [0.33912462, 0.34762183]\n",
      "16400 [0.3263467, 0.32971048]\n",
      "16500 [0.3318899, 0.33518159]\n",
      "16600 [0.33923846, 0.3489657]\n",
      "16700 [0.32867631, 0.33116129]\n",
      "16800 [0.32538378, 0.3287518]\n",
      "16900 [0.32754815, 0.33133]\n",
      "17000 [0.33629206, 0.33896628]\n",
      "17100 [0.32989374, 0.33320028]\n",
      "17200 [0.32985136, 0.33295476]\n",
      "17300 [0.31684846, 0.31950459]\n",
      "17400 [0.33598435, 0.33927181]\n",
      "17500 [0.31600198, 0.3190842]\n",
      "17600 [0.32230714, 0.32666826]\n",
      "17700 [0.33263221, 0.33616218]\n",
      "17800 [0.33538952, 0.34063298]\n",
      "17900 [0.34191632, 0.34535217]\n",
      "18000 [0.317635, 0.32301924]\n",
      "18100 [0.33512139, 0.33734658]\n",
      "18200 [0.33864227, 0.34089538]\n",
      "18300 [0.33069092, 0.33420414]\n",
      "18400 [0.32106146, 0.32332182]\n",
      "18500 [0.32766262, 0.33137074]\n",
      "18600 [0.32339248, 0.32609937]\n",
      "18700 [0.33954236, 0.34199613]\n",
      "18800 [0.34823108, 0.35123089]\n",
      "18900 [0.34063953, 0.34566495]\n",
      "19000 [0.3342602, 0.33703253]\n",
      "19100 [0.31403682, 0.31614533]\n",
      "19200 [0.33588907, 0.33784214]\n",
      "19300 [0.34351474, 0.34639817]\n",
      "19400 [0.34886125, 0.35445356]\n",
      "19500 [0.30782899, 0.31044599]\n",
      "19600 [0.31060737, 0.31364825]\n",
      "19700 [0.32546398, 0.33045939]\n",
      "19800 [0.32772881, 0.33039472]\n",
      "19900 [0.32543135, 0.32719076]\n",
      "20000 [0.3265062, 0.32833007]\n",
      "20100 [0.33435124, 0.33731174]\n",
      "20200 [0.32725698, 0.33014157]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 15 17:22:08 2017\n",
    "\n",
    "@author: cnbyb\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "tf.Session(config=config)\n",
    "import numpy as np\n",
    "\n",
    "def g(x):\n",
    "    return tf.nn.relu(x) ** 3\n",
    "    #return 1.0507*(tf.nn.relu(x)+1.6733*tf.exp(-tf.nn.relu(-x))-1.6733)\n",
    "    \n",
    "def block(x,size,scope):\n",
    "    initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1)\n",
    "    with tf.variable_scope(scope):\n",
    "        W1 = tf.get_variable('w1', shape=size, dtype='float', initializer=initializer)\n",
    "        b1 = tf.get_variable('b1', dtype='float', initializer=tf.zeros([size[1]]))\n",
    "        W2 = tf.get_variable('w2', shape=[size[1], size[1]], dtype='float',initializer=initializer)\n",
    "        b2 = tf.get_variable('b2', dtype='float', initializer=tf.zeros([size[1]]))\n",
    "        y = tf.matmul(x,W1) + b1\n",
    "        y = g(y)\n",
    "        y = tf.matmul(y,W2) + b2\n",
    "        \n",
    "        if size[0] < size[1]:\n",
    "            y = g(y) + tf.concat([x, tf.matmul(x, tf.zeros([2,size[1] - size[0]]))], axis=-1)\n",
    "        elif size[0] == size[1]:\n",
    "            y=g(y) + x\n",
    "        else:\n",
    "            w_res = tf.get_variable('w_res', shape=[size[0], size[1]], dtype=tf.float32)\n",
    "            y = g(y) + tf.matmul(x,w_res)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def func(x,is_training):\n",
    "    size = 10\n",
    "    y = block(x,[2,size], 'block1')\n",
    "    y = block(y,[size,size], 'block2')\n",
    "    y = block(y,[size,size], 'block3')\n",
    "    y = block(y,[size,size], 'block4')\n",
    "    y = block(y,[size,size], 'block5')\n",
    "    #y=block(y,[size,size],'block6')\n",
    "    #y=block(y,[size,size],'block7')\n",
    "    #y=block(y,[size,size],'block8')\n",
    "    W = tf.get_variable(\"w\", shape=[size, 1], dtype='float')\n",
    "    b = tf.get_variable(\"b\", shape=[1], dtype='float')\n",
    "    u = tf.matmul(y, W) + b\n",
    "    return u\n",
    "def pfunc(x):\n",
    "    return x[:,0] * x[:,1]\n",
    "\n",
    "#sample node points in Omega, nodeb*4 points on the border of Omega\n",
    "def get_batch(node,nodeb):\n",
    "    x = np.random.random([node, 2])\n",
    "    xblist = []\n",
    "    for d in range(2):\n",
    "        xb0 = np.random.random([nodeb, 2])\n",
    "        xb0[:, d] = 0.0\n",
    "        xb1 = np.random.random([nodeb, 2])\n",
    "        xb1[:, d] = 1.0\n",
    "        xblist.append(xb0)\n",
    "        xblist.append(xb1)\n",
    "    return x, np.concatenate(xblist, axis=0)\n",
    "\n",
    "node = 500\n",
    "nodeb = 100\n",
    "#x = tf.placeholder(tf.float32, [(2*node+1)**2,2])\n",
    "x = tf.placeholder(tf.float32, [None,2])\n",
    "#xb = tf.placeholder(tf.float32, [nodeb*5,2])\n",
    "xb = tf.placeholder(tf.float32, [None,2])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "with tf.variable_scope('y') as scope:\n",
    "    u=func(x, is_training)\n",
    "    scope.reuse_variables()\n",
    "    ub=func(xb, is_training)\n",
    "    \n",
    "pub = pfunc(xb)\n",
    "pu = pfunc(x)\n",
    "ux = tf.gradients(u,x)[0]\n",
    "#beta = tf.get_variable(name='beta', 1e2, trainable=False)\n",
    "\n",
    "beta = 5e2\n",
    "Iusum = tf.reduce_mean(tf.reduce_sum(ux * ux, axis=1)) / 2\n",
    "loss = Iusum + beta * tf.reduce_mean((ub[:, 0] - pub) ** 2)\n",
    "global_step = tf.get_variable(name='global_step', initializer=0, trainable=False)\n",
    "lr = tf.train.exponential_decay(1e-2, global_step, 10000, 0.5, staircase=True)\n",
    "op = tf.assign_add(global_step, 1)\n",
    "#opt = tf.train.AdamOptimizer(lr, 0.99)\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "with tf.control_dependencies([op]):\n",
    "    grad = opt.compute_gradients(loss)\n",
    "    train_op = opt.apply_gradients(grad)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for i in range(50000):\n",
    "    x_batch, xb_batch = get_batch(node, nodeb)\n",
    "    sess.run(train_op,feed_dict = {x: x_batch, xb: xb_batch, is_training: True})\n",
    "    if ((i%100)==0):\n",
    "        #每一百次输出：迭代次数；I(u)减去正则项的值；I(u)的值\n",
    "        print(i, sess.run([Iusum, loss], feed_dict={x: x_batch, xb: xb_batch, is_training: False}))\n",
    "        \n",
    "node = 10240\n",
    "xgrid = np.random.random([node, 2])\n",
    "\n",
    "u_solve = sess.run(u, feed_dict = {x: xgrid, is_training: False})\n",
    "pu = pfunc(xgrid)\n",
    "#pu=np.reshape(pfunc_new(xgrid),[2*node+1,2*node+1])\n",
    "\n",
    "#相对误差\n",
    "print(np.linalg.norm(np.reshape(u_solve[:,0]-pu, [-1]), ord=2)\n",
    "      / np.linalg.norm(np.reshape(pu, [-1]), ord=2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
