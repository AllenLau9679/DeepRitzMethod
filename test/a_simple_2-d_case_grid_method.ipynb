{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a simple case\n",
    "Consider the following Possion Equation\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\Delta u = 1\\qquad &u\\in\\Omega\\\\\n",
    "    u = 0\\qquad &u\\in\\partial\\Omega.\n",
    "\\end{cases}$$\n",
    "Here $\\Omega = \\{(x, y)|x^2+y^2 < 1\\}$\n",
    "\n",
    "The exact solution to this problem is $$u = \\frac{1}{4}(x^2+y^2-1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "#learning rate decay\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "class DeepRitzNet(torch.nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super(DeepRitzNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(m,m)\n",
    "        self.linear2 = torch.nn.Linear(m,m)\n",
    "        self.linear3 = torch.nn.Linear(m,m)\n",
    "        self.linear4 = torch.nn.Linear(m,m)\n",
    "        self.linear5 = torch.nn.Linear(m,m)\n",
    "        self.linear6 = torch.nn.Linear(m,m)\n",
    "        \n",
    "        self.linear7 = torch.nn.Linear(m,1)\n",
    "        '''\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear4.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear5.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear6.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        torch.nn.init.xavier_uniform_(self.linear7.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = y + F.relu(self.linear2(F.relu(self.linear1(y))))\n",
    "        y = y + F.relu(self.linear4(F.relu(self.linear3(y))))\n",
    "        y = y + F.relu(self.linear6(F.relu(self.linear5(y))))\n",
    "        output = F.relu(self.linear7(y))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_graph(mod):\n",
    "    points = np.arange(-1, 1, 0.01)\n",
    "    xs, ys = np.meshgrid(points, points)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xl, yl = xs.size()\n",
    "    z = np.zeros((xl, yl))\n",
    "    for i in range(xl):\n",
    "        for j in range(yl):      \n",
    "            re = np.zeros(m)\n",
    "            re[0] = xs[i, j]\n",
    "            re[1] = ys[i, j]\n",
    "            re = torch.tensor(re)        \n",
    "            z[i, j] = mod(re.float()).item() + U_groundtruth(re)\n",
    "    \n",
    "    plt.imshow(z, cmap=cm.hot)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    #plt.savefig(\"loss_1.eps\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_loss(mod):\n",
    "    points = np.arange(-1, 1, 0.1)\n",
    "    xs, ys = np.meshgrid(points, points)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xl, yl = xs.size()\n",
    "    z = np.zeros((xl, yl))\n",
    "    mmm = 0\n",
    "    t = 0\n",
    "    for i in range(xl):\n",
    "        for j in range(yl):      \n",
    "            re = np.zeros(m)\n",
    "            re[0] = xs[i, j]\n",
    "            re[1] = ys[i, j]\n",
    "            re = torch.tensor(re)        \n",
    "            z[i, j] = mod(re.float()).item() + U_groundtruth(re)\n",
    "            #?????????\n",
    "            if re[0] ** 2 + re[1] ** 2 < 1 : \n",
    "                mmm += abs(z[i, j])\n",
    "                t += 1\n",
    "    return mmm / t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#相对误差\n",
    "#余冰的例子里面是\n",
    "#print(np.linalg.norm(np.reshape(u_solve[:,0]-pu,[-1]),ord=2)/np.linalg.norm(np.reshape(pu,[-1]),ord=2))\n",
    "def relative_err(mod):\n",
    "    points = np.arange(-1, 1, 0.1)\n",
    "    xs, ys = np.meshgrid(points, points)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xl, yl = xs.size()\n",
    "    z = np.zeros((xl, yl))\n",
    "    w = np.zeros((xl, yl))\n",
    "    t = 0\n",
    "    for i in range(xl):\n",
    "        for j in range(yl):      \n",
    "            re = np.zeros(m)\n",
    "            re[0] = xs[i, j]\n",
    "            re[1] = ys[i, j]\n",
    "            re = torch.tensor(re)\n",
    "            if re[0] ** 2 + re[1] ** 2 < 1 :\n",
    "                z[i, j] = mod(re.float()).item() + U_groundtruth(re)\n",
    "                w[i, j] = U_groundtruth(re)\n",
    "                t += 1\n",
    "    z = z ** 2\n",
    "    w = w ** 2\n",
    "    return np.sum(z) / np.sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#U_groundtruth = 1/4*(x^2+y^2)-1/4\n",
    "#take in a (m,) tensor (x, y, ...)\n",
    "def U_groundtruth(t):\n",
    "    #re = 0\n",
    "    re = (t[0] ** 2 + t[1] ** 2 - 1).item() / 4\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(mod):\n",
    "    draw_graph(mod)\n",
    "    print(cal_loss(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "learning_rate = 0.01\n",
    "iterations = 400  #default 10000\n",
    "print_every_iter = 100\n",
    "beta = 500 #coefficient for the regularization term in the loss expression, is set to be 1000 in section 3.1\n",
    "n1 = 1000 #number of points in (0,1)^m\n",
    "n2 = 100  #number of points on the border of (0,1)^m\n",
    "n3 = 100  #number of points used for evaluating the error\n",
    "gamma = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  epoch, loss:  0.003757581\n",
      "0  epoch, regularization loss:  0.0014568424\n",
      "0  loss to real solution:  0.12585706354720796\n",
      "1  epoch, loss:  0.0017227847\n",
      "1  epoch, regularization loss:  0.0049991105\n",
      "1  loss to real solution:  0.12494390999197574\n",
      "2  epoch, loss:  -0.006674614\n",
      "2  epoch, regularization loss:  0.010949647\n",
      "2  loss to real solution:  0.12307167222837155\n",
      "3  epoch, loss:  -0.02460782\n",
      "3  epoch, regularization loss:  0.020303011\n",
      "3  loss to real solution:  0.12117159876601107\n",
      "4  epoch, loss:  -0.05633974\n",
      "4  epoch, regularization loss:  0.034012053\n",
      "4  loss to real solution:  0.11934473653698269\n",
      "5  epoch, loss:  -0.10548175\n",
      "5  epoch, regularization loss:  0.053524155\n",
      "5  loss to real solution:  0.12056768686058446\n",
      "6  epoch, loss:  -0.15654023\n",
      "6  epoch, regularization loss:  0.07246395\n",
      "6  loss to real solution:  0.1259098148690926\n",
      "7  epoch, loss:  -0.22613847\n",
      "7  epoch, regularization loss:  0.09942548\n",
      "7  loss to real solution:  0.14328349177668728\n",
      "8  epoch, loss:  -0.30700633\n",
      "8  epoch, regularization loss:  0.13859774\n",
      "8  loss to real solution:  0.20106765162714804\n",
      "9  epoch, loss:  -0.38828668\n",
      "9  epoch, regularization loss:  0.19086076\n",
      "9  loss to real solution:  0.27516464395155094\n",
      "10  epoch, loss:  -0.46355343\n",
      "10  epoch, regularization loss:  0.25222751\n",
      "10  loss to real solution:  0.34491455065859083\n",
      "11  epoch, loss:  -0.5243844\n",
      "11  epoch, regularization loss:  0.3112282\n",
      "11  loss to real solution:  0.40229771863225944\n",
      "12  epoch, loss:  -0.5629877\n",
      "12  epoch, regularization loss:  0.3528489\n",
      "12  loss to real solution:  0.4394891876269767\n",
      "13  epoch, loss:  -0.57534635\n",
      "13  epoch, regularization loss:  0.3674107\n",
      "13  loss to real solution:  0.45242723247245953\n",
      "14  epoch, loss:  -0.5643197\n",
      "14  epoch, regularization loss:  0.3543352\n",
      "14  loss to real solution:  0.44327165593288326\n",
      "15  epoch, loss:  -0.536763\n",
      "15  epoch, regularization loss:  0.32097998\n",
      "15  loss to real solution:  0.4180764139043557\n",
      "16  epoch, loss:  -0.49932703\n",
      "16  epoch, regularization loss:  0.27748412\n",
      "16  loss to real solution:  0.38337709809422865\n",
      "17  epoch, loss:  -0.4587105\n",
      "17  epoch, regularization loss:  0.23268828\n",
      "17  loss to real solution:  0.34519821040883325\n",
      "18  epoch, loss:  -0.42098263\n",
      "18  epoch, regularization loss:  0.19356588\n",
      "18  loss to real solution:  0.3096290689725968\n",
      "19  epoch, loss:  -0.39221162\n",
      "19  epoch, regularization loss:  0.16464546\n",
      "19  loss to real solution:  0.28287970075461644\n",
      "20  epoch, loss:  -0.3768543\n",
      "20  epoch, regularization loss:  0.14815353\n",
      "20  loss to real solution:  0.2687860532742221\n",
      "21  epoch, loss:  -0.37296095\n",
      "21  epoch, regularization loss:  0.14133853\n",
      "21  loss to real solution:  0.26594619021346716\n",
      "22  epoch, loss:  -0.37946382\n",
      "22  epoch, regularization loss:  0.14337145\n",
      "22  loss to real solution:  0.27328621430412364\n",
      "23  epoch, loss:  -0.39566827\n",
      "23  epoch, regularization loss:  0.155311\n",
      "23  loss to real solution:  0.2898588170920921\n",
      "24  epoch, loss:  -0.41543716\n",
      "24  epoch, regularization loss:  0.1725157\n",
      "24  loss to real solution:  0.310077623247717\n",
      "25  epoch, loss:  -0.43754587\n",
      "25  epoch, regularization loss:  0.1941735\n",
      "25  loss to real solution:  0.3323605222487373\n",
      "26  epoch, loss:  -0.45499963\n",
      "26  epoch, regularization loss:  0.21348512\n",
      "26  loss to real solution:  0.3509506682835974\n",
      "27  epoch, loss:  -0.46569678\n",
      "27  epoch, regularization loss:  0.22657274\n",
      "27  loss to real solution:  0.3634041617690941\n",
      "28  epoch, loss:  -0.46776205\n",
      "28  epoch, regularization loss:  0.23122148\n",
      "28  loss to real solution:  0.36860733591475286\n",
      "29  epoch, loss:  -0.46168762\n",
      "29  epoch, regularization loss:  0.22662432\n",
      "29  loss to real solution:  0.3662393753321608\n",
      "30  epoch, loss:  -0.4482402\n",
      "30  epoch, regularization loss:  0.21380478\n",
      "30  loss to real solution:  0.35682388622277794\n",
      "31  epoch, loss:  -0.43021154\n",
      "31  epoch, regularization loss:  0.19645369\n",
      "31  loss to real solution:  0.34351257668047464\n",
      "32  epoch, loss:  -0.41081464\n",
      "32  epoch, regularization loss:  0.1781925\n",
      "32  loss to real solution:  0.32916968638873945\n",
      "33  epoch, loss:  -0.3939509\n",
      "33  epoch, regularization loss:  0.16241817\n",
      "33  loss to real solution:  0.31681886238300533\n",
      "34  epoch, loss:  -0.38209534\n",
      "34  epoch, regularization loss:  0.15180704\n",
      "34  loss to real solution:  0.3092899662658716\n",
      "35  epoch, loss:  -0.3767792\n",
      "35  epoch, regularization loss:  0.14740817\n",
      "35  loss to real solution:  0.3077616548768194\n",
      "36  epoch, loss:  -0.37821862\n",
      "36  epoch, regularization loss:  0.14913024\n",
      "36  loss to real solution:  0.31208539487847947\n",
      "37  epoch, loss:  -0.3919878\n",
      "37  epoch, regularization loss:  0.1630549\n",
      "37  loss to real solution:  0.32630398974541297\n",
      "38  epoch, loss:  -0.40784103\n",
      "38  epoch, regularization loss:  0.18073598\n",
      "38  loss to real solution:  0.34232818505005047\n",
      "39  epoch, loss:  -0.41978297\n",
      "39  epoch, regularization loss:  0.19476373\n",
      "39  loss to real solution:  0.3536456425626945\n",
      "40  epoch, loss:  -0.42459115\n",
      "40  epoch, regularization loss:  0.20154099\n",
      "40  loss to real solution:  0.35820201086844655\n",
      "41  epoch, loss:  -0.4204724\n",
      "41  epoch, regularization loss:  0.19947912\n",
      "41  loss to real solution:  0.3539797901302288\n",
      "42  epoch, loss:  -0.41073346\n",
      "42  epoch, regularization loss:  0.19059263\n",
      "42  loss to real solution:  0.34379108513283185\n",
      "43  epoch, loss:  -0.39626992\n",
      "43  epoch, regularization loss:  0.17751989\n",
      "43  loss to real solution:  0.32994890212629385\n",
      "44  epoch, loss:  -0.38299808\n",
      "44  epoch, regularization loss:  0.16548367\n",
      "44  loss to real solution:  0.3161622061568441\n",
      "45  epoch, loss:  -0.37228525\n",
      "45  epoch, regularization loss:  0.15723094\n",
      "45  loss to real solution:  0.30603439334121163\n",
      "46  epoch, loss:  -0.3661424\n",
      "46  epoch, regularization loss:  0.15314636\n",
      "46  loss to real solution:  0.300930577167744\n",
      "47  epoch, loss:  -0.36466137\n",
      "47  epoch, regularization loss:  0.15301584\n",
      "47  loss to real solution:  0.300590091671591\n",
      "48  epoch, loss:  -0.36599728\n",
      "48  epoch, regularization loss:  0.15575029\n",
      "48  loss to real solution:  0.30363928418450803\n",
      "49  epoch, loss:  -0.36917982\n",
      "49  epoch, regularization loss:  0.15989652\n",
      "49  loss to real solution:  0.30792757552153044\n",
      "50  epoch, loss:  -0.37133536\n",
      "50  epoch, regularization loss:  0.16305193\n",
      "50  loss to real solution:  0.31109624525741747\n",
      "51  epoch, loss:  -0.36200607\n",
      "51  epoch, regularization loss:  0.15398727\n",
      "51  loss to real solution:  0.30128956501890225\n",
      "52  epoch, loss:  -0.35084194\n",
      "52  epoch, regularization loss:  0.14425856\n",
      "52  loss to real solution:  0.2901136761079648\n",
      "53  epoch, loss:  -0.3508041\n",
      "53  epoch, regularization loss:  0.14751713\n",
      "53  loss to real solution:  0.28839520194522833\n",
      "54  epoch, loss:  -0.35061204\n",
      "54  epoch, regularization loss:  0.15064617\n",
      "54  loss to real solution:  0.28731574884373273\n",
      "55  epoch, loss:  -0.351462\n",
      "55  epoch, regularization loss:  0.15385084\n",
      "55  loss to real solution:  0.2874031904387704\n",
      "56  epoch, loss:  -0.35260835\n",
      "56  epoch, regularization loss:  0.15675813\n",
      "56  loss to real solution:  0.2879636314482551\n",
      "57  epoch, loss:  -0.3540528\n",
      "57  epoch, regularization loss:  0.15867162\n",
      "57  loss to real solution:  0.28797111585592533\n",
      "58  epoch, loss:  -0.3522197\n",
      "58  epoch, regularization loss:  0.15747613\n",
      "58  loss to real solution:  0.2851641578651318\n",
      "59  epoch, loss:  -0.34708405\n",
      "59  epoch, regularization loss:  0.15297078\n",
      "59  loss to real solution:  0.279406569966166\n",
      "60  epoch, loss:  -0.34020194\n",
      "60  epoch, regularization loss:  0.14653517\n",
      "60  loss to real solution:  0.27167899288165226\n",
      "61  epoch, loss:  -0.3334357\n",
      "61  epoch, regularization loss:  0.14065193\n",
      "61  loss to real solution:  0.26522698763290786\n",
      "62  epoch, loss:  -0.3255012\n",
      "62  epoch, regularization loss:  0.1338703\n",
      "62  loss to real solution:  0.25698715639651015\n",
      "63  epoch, loss:  -0.31865492\n",
      "63  epoch, regularization loss:  0.1282584\n",
      "63  loss to real solution:  0.2498529358845432\n",
      "64  epoch, loss:  -0.3148963\n",
      "64  epoch, regularization loss:  0.12572353\n",
      "64  loss to real solution:  0.24579896343865962\n",
      "65  epoch, loss:  -0.31267312\n",
      "65  epoch, regularization loss:  0.12483353\n",
      "65  loss to real solution:  0.24332006117155314\n",
      "66  epoch, loss:  -0.31191918\n",
      "66  epoch, regularization loss:  0.12542023\n",
      "66  loss to real solution:  0.24257585544869822\n",
      "67  epoch, loss:  -0.31217146\n",
      "67  epoch, regularization loss:  0.12658617\n",
      "67  loss to real solution:  0.24321551869728167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68  epoch, loss:  -0.31303304\n",
      "68  epoch, regularization loss:  0.12824012\n",
      "68  loss to real solution:  0.24474276994777264\n",
      "69  epoch, loss:  -0.3133995\n",
      "69  epoch, regularization loss:  0.12943059\n",
      "69  loss to real solution:  0.24598872680372746\n",
      "70  epoch, loss:  -0.31168887\n",
      "70  epoch, regularization loss:  0.12944536\n",
      "70  loss to real solution:  0.24600985631490435\n",
      "71  epoch, loss:  -0.30720967\n",
      "71  epoch, regularization loss:  0.12664717\n",
      "71  loss to real solution:  0.24300328696847337\n",
      "72  epoch, loss:  -0.30309236\n",
      "72  epoch, regularization loss:  0.124337934\n",
      "72  loss to real solution:  0.2402248733258323\n",
      "73  epoch, loss:  -0.29955977\n",
      "73  epoch, regularization loss:  0.122458495\n",
      "73  loss to real solution:  0.23763043790001578\n",
      "74  epoch, loss:  -0.2968656\n",
      "74  epoch, regularization loss:  0.120770946\n",
      "74  loss to real solution:  0.23512396680772105\n",
      "75  epoch, loss:  -0.2950242\n",
      "75  epoch, regularization loss:  0.119422145\n",
      "75  loss to real solution:  0.23281713088417352\n",
      "76  epoch, loss:  -0.2935691\n",
      "76  epoch, regularization loss:  0.11848822\n",
      "76  loss to real solution:  0.23076737563901384\n",
      "77  epoch, loss:  -0.29244223\n",
      "77  epoch, regularization loss:  0.11792762\n",
      "77  loss to real solution:  0.22883948458160994\n",
      "78  epoch, loss:  -0.29415545\n",
      "78  epoch, regularization loss:  0.120183714\n",
      "78  loss to real solution:  0.22925213046012574\n",
      "79  epoch, loss:  -0.2936823\n",
      "79  epoch, regularization loss:  0.120827235\n",
      "79  loss to real solution:  0.227804525916216\n",
      "80  epoch, loss:  -0.2945469\n",
      "80  epoch, regularization loss:  0.123398475\n",
      "80  loss to real solution:  0.22765235707882508\n",
      "81  epoch, loss:  -0.29089573\n",
      "81  epoch, regularization loss:  0.121465564\n",
      "81  loss to real solution:  0.22306011041835958\n",
      "82  epoch, loss:  -0.28279188\n",
      "82  epoch, regularization loss:  0.1151258\n",
      "82  loss to real solution:  0.21444837475512957\n",
      "83  epoch, loss:  -0.2737997\n",
      "83  epoch, regularization loss:  0.107632525\n",
      "83  loss to real solution:  0.20501392006107466\n",
      "84  epoch, loss:  -0.26835078\n",
      "84  epoch, regularization loss:  0.102850474\n",
      "84  loss to real solution:  0.1986173102863348\n",
      "85  epoch, loss:  -0.26601917\n",
      "85  epoch, regularization loss:  0.10123646\n",
      "85  loss to real solution:  0.1957340131196945\n",
      "86  epoch, loss:  -0.2691123\n",
      "86  epoch, regularization loss:  0.10518512\n",
      "86  loss to real solution:  0.1979952386491168\n",
      "87  epoch, loss:  -0.2734778\n",
      "87  epoch, regularization loss:  0.11040486\n",
      "87  loss to real solution:  0.20156310474374284\n",
      "88  epoch, loss:  -0.27498597\n",
      "88  epoch, regularization loss:  0.1127459\n",
      "88  loss to real solution:  0.2026450917613468\n",
      "89  epoch, loss:  -0.28666797\n",
      "89  epoch, regularization loss:  0.12585025\n",
      "89  loss to real solution:  0.21157784582334316\n",
      "90  epoch, loss:  -0.28559595\n",
      "90  epoch, regularization loss:  0.12648574\n",
      "90  loss to real solution:  0.20806799181787913\n",
      "91  epoch, loss:  -0.27076173\n",
      "91  epoch, regularization loss:  0.11274538\n",
      "91  loss to real solution:  0.19223197604298972\n",
      "92  epoch, loss:  -0.25182307\n",
      "92  epoch, regularization loss:  0.09530166\n",
      "92  loss to real solution:  0.1731510351095169\n",
      "93  epoch, loss:  -0.24191691\n",
      "93  epoch, regularization loss:  0.08636625\n",
      "93  loss to real solution:  0.1635804279306694\n",
      "94  epoch, loss:  -0.24564001\n",
      "94  epoch, regularization loss:  0.089494064\n",
      "94  loss to real solution:  0.1679569516304605\n",
      "95  epoch, loss:  -0.2577311\n",
      "95  epoch, regularization loss:  0.10092094\n",
      "95  loss to real solution:  0.18049225895926116\n",
      "96  epoch, loss:  -0.26602992\n",
      "96  epoch, regularization loss:  0.11054777\n",
      "96  loss to real solution:  0.19076324361696886\n",
      "97  epoch, loss:  -0.26332423\n",
      "97  epoch, regularization loss:  0.108746864\n",
      "97  loss to real solution:  0.1905351617481931\n",
      "98  epoch, loss:  -0.24956189\n",
      "98  epoch, regularization loss:  0.09594687\n",
      "98  loss to real solution:  0.18024062209190664\n",
      "99  epoch, loss:  -0.23428842\n",
      "99  epoch, regularization loss:  0.08253324\n",
      "99  loss to real solution:  0.16866108333949492\n",
      "100  epoch, loss:  -0.22649987\n",
      "100  epoch, regularization loss:  0.07668243\n",
      "100  loss to real solution:  0.16416202815782602\n",
      "101  epoch, loss:  -0.23600557\n",
      "101  epoch, regularization loss:  0.08678269\n",
      "101  loss to real solution:  0.17562813438403263\n",
      "102  epoch, loss:  -0.24727142\n",
      "102  epoch, regularization loss:  0.099381536\n",
      "102  loss to real solution:  0.18855552839696219\n",
      "103  epoch, loss:  -0.24763101\n",
      "103  epoch, regularization loss:  0.10071718\n",
      "103  loss to real solution:  0.18923715360294974\n",
      "104  epoch, loss:  -0.23920396\n",
      "104  epoch, regularization loss:  0.09199155\n",
      "104  loss to real solution:  0.1806193207774515\n",
      "105  epoch, loss:  -0.22795512\n",
      "105  epoch, regularization loss:  0.08147615\n",
      "105  loss to real solution:  0.16977742103135088\n",
      "106  epoch, loss:  -0.22525385\n",
      "106  epoch, regularization loss:  0.07983596\n",
      "106  loss to real solution:  0.16647775053977967\n",
      "107  epoch, loss:  -0.22908115\n",
      "107  epoch, regularization loss:  0.08485574\n",
      "107  loss to real solution:  0.16975051135495547\n",
      "108  epoch, loss:  -0.23455757\n",
      "108  epoch, regularization loss:  0.09167681\n",
      "108  loss to real solution:  0.17462214491758318\n",
      "109  epoch, loss:  -0.23606417\n",
      "109  epoch, regularization loss:  0.09399388\n",
      "109  loss to real solution:  0.17552657027336566\n",
      "110  epoch, loss:  -0.2302618\n",
      "110  epoch, regularization loss:  0.089557245\n",
      "110  loss to real solution:  0.17072364591708908\n",
      "111  epoch, loss:  -0.23074383\n",
      "111  epoch, regularization loss:  0.090419345\n",
      "111  loss to real solution:  0.1678943657472586\n",
      "112  epoch, loss:  -0.2254291\n",
      "112  epoch, regularization loss:  0.085426085\n",
      "112  loss to real solution:  0.16016073069388465\n",
      "113  epoch, loss:  -0.21815552\n",
      "113  epoch, regularization loss:  0.07847158\n",
      "113  loss to real solution:  0.1510267893466919\n",
      "114  epoch, loss:  -0.21386723\n",
      "114  epoch, regularization loss:  0.07478675\n",
      "114  loss to real solution:  0.1455379614741856\n",
      "115  epoch, loss:  -0.21500492\n",
      "115  epoch, regularization loss:  0.07678505\n",
      "115  loss to real solution:  0.14624488755821036\n",
      "116  epoch, loss:  -0.21891083\n",
      "116  epoch, regularization loss:  0.0815784\n",
      "116  loss to real solution:  0.14968747197240112\n",
      "117  epoch, loss:  -0.2198087\n",
      "117  epoch, regularization loss:  0.08354709\n",
      "117  loss to real solution:  0.15005351803885397\n",
      "118  epoch, loss:  -0.21718074\n",
      "118  epoch, regularization loss:  0.08207795\n",
      "118  loss to real solution:  0.14727311271946528\n",
      "119  epoch, loss:  -0.21186721\n",
      "119  epoch, regularization loss:  0.07725213\n",
      "119  loss to real solution:  0.14189345918667665\n",
      "120  epoch, loss:  -0.20878267\n",
      "120  epoch, regularization loss:  0.07470231\n",
      "120  loss to real solution:  0.13876029700136647\n",
      "121  epoch, loss:  -0.20819594\n",
      "121  epoch, regularization loss:  0.07497394\n",
      "121  loss to real solution:  0.13818228488758066\n",
      "122  epoch, loss:  -0.20905197\n",
      "122  epoch, regularization loss:  0.07651188\n",
      "122  loss to real solution:  0.13881263577286454\n",
      "123  epoch, loss:  -0.20902647\n",
      "123  epoch, regularization loss:  0.076947466\n",
      "123  loss to real solution:  0.13853089970982727\n",
      "124  epoch, loss:  -0.20592847\n",
      "124  epoch, regularization loss:  0.07487835\n",
      "124  loss to real solution:  0.1360809463741695\n",
      "125  epoch, loss:  -0.20157306\n",
      "125  epoch, regularization loss:  0.07172036\n",
      "125  loss to real solution:  0.13286894775088579\n",
      "126  epoch, loss:  -0.19936968\n",
      "126  epoch, regularization loss:  0.07030207\n",
      "126  loss to real solution:  0.13170541707534117\n",
      "127  epoch, loss:  -0.1961694\n",
      "127  epoch, regularization loss:  0.06724314\n",
      "127  loss to real solution:  0.12484704834471949\n",
      "128  epoch, loss:  -0.19600159\n",
      "128  epoch, regularization loss:  0.06847837\n",
      "128  loss to real solution:  0.12306455428960625\n",
      "129  epoch, loss:  -0.21241446\n",
      "129  epoch, regularization loss:  0.08662047\n",
      "129  loss to real solution:  0.1381524904709537\n",
      "130  epoch, loss:  -0.20986103\n",
      "130  epoch, regularization loss:  0.08510885\n",
      "130  loss to real solution:  0.13350299671148566\n",
      "131  epoch, loss:  -0.18061277\n",
      "131  epoch, regularization loss:  0.0579136\n",
      "131  loss to real solution:  0.10288129394364895\n",
      "132  epoch, loss:  -0.1762739\n",
      "132  epoch, regularization loss:  0.056650102\n",
      "132  loss to real solution:  0.09692290817809644\n",
      "133  epoch, loss:  -0.20203958\n",
      "133  epoch, regularization loss:  0.08277583\n",
      "133  loss to real solution:  0.11947593312938119\n",
      "134  epoch, loss:  -0.21121998\n",
      "134  epoch, regularization loss:  0.09461897\n",
      "134  loss to real solution:  0.12706439491349408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135  epoch, loss:  -0.18511675\n",
      "135  epoch, regularization loss:  0.068070784\n",
      "135  loss to real solution:  0.10102990307608603\n",
      "136  epoch, loss:  -0.16735941\n",
      "136  epoch, regularization loss:  0.05208835\n",
      "136  loss to real solution:  0.08307033642695265\n",
      "137  epoch, loss:  -0.18566303\n",
      "137  epoch, regularization loss:  0.06912157\n",
      "137  loss to real solution:  0.10043656193175118\n",
      "138  epoch, loss:  -0.19797675\n",
      "138  epoch, regularization loss:  0.08361306\n",
      "138  loss to real solution:  0.11340510102138643\n",
      "139  epoch, loss:  -0.17639354\n",
      "139  epoch, regularization loss:  0.061338186\n",
      "139  loss to real solution:  0.0933985694054622\n",
      "140  epoch, loss:  -0.16324563\n",
      "140  epoch, regularization loss:  0.048479207\n",
      "140  loss to real solution:  0.08109942940653711\n",
      "141  epoch, loss:  -0.18086724\n",
      "141  epoch, regularization loss:  0.06564093\n",
      "141  loss to real solution:  0.10021935331093156\n",
      "142  epoch, loss:  -0.20779179\n",
      "142  epoch, regularization loss:  0.096356556\n",
      "142  loss to real solution:  0.12509830971239466\n",
      "143  epoch, loss:  -0.17658292\n",
      "143  epoch, regularization loss:  0.06383688\n",
      "143  loss to real solution:  0.09388387883207808\n",
      "144  epoch, loss:  -0.1533601\n",
      "144  epoch, regularization loss:  0.045536675\n",
      "144  loss to real solution:  0.0700120128657657\n",
      "145  epoch, loss:  -0.18389405\n",
      "145  epoch, regularization loss:  0.07361951\n",
      "145  loss to real solution:  0.09900103408424035\n",
      "146  epoch, loss:  -0.20194392\n",
      "146  epoch, regularization loss:  0.093138404\n",
      "146  loss to real solution:  0.11622445088490799\n",
      "147  epoch, loss:  -0.16839431\n",
      "147  epoch, regularization loss:  0.056103468\n",
      "147  loss to real solution:  0.08392872742422142\n",
      "148  epoch, loss:  -0.15430868\n",
      "148  epoch, regularization loss:  0.043735385\n",
      "148  loss to real solution:  0.07201078519368861\n",
      "149  epoch, loss:  -0.17586532\n",
      "149  epoch, regularization loss:  0.06576863\n",
      "149  loss to real solution:  0.09568920750326668\n",
      "150  epoch, loss:  -0.1801188\n",
      "150  epoch, regularization loss:  0.07144254\n",
      "150  loss to real solution:  0.10171417872430427\n",
      "151  epoch, loss:  -0.17903164\n",
      "151  epoch, regularization loss:  0.068807386\n",
      "151  loss to real solution:  0.10116802785653393\n",
      "152  epoch, loss:  -0.1526078\n",
      "152  epoch, regularization loss:  0.043644883\n",
      "152  loss to real solution:  0.07508557641621187\n",
      "153  epoch, loss:  -0.15734114\n",
      "153  epoch, regularization loss:  0.0510919\n",
      "153  loss to real solution:  0.08036102478718835\n",
      "154  epoch, loss:  -0.17985417\n",
      "154  epoch, regularization loss:  0.07684015\n",
      "154  loss to real solution:  0.1023969073349257\n",
      "155  epoch, loss:  -0.1708447\n",
      "155  epoch, regularization loss:  0.0681997\n",
      "155  loss to real solution:  0.0929713126509136\n",
      "156  epoch, loss:  -0.14883849\n",
      "156  epoch, regularization loss:  0.047054786\n",
      "156  loss to real solution:  0.07081385076237644\n",
      "157  epoch, loss:  -0.15355146\n",
      "157  epoch, regularization loss:  0.05119598\n",
      "157  loss to real solution:  0.07459845072490992\n",
      "158  epoch, loss:  -0.16615497\n",
      "158  epoch, regularization loss:  0.066087976\n",
      "158  loss to real solution:  0.0872218664397764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-15b5e9f80928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mx_input_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mx_input_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[0mx_input_grad_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 \u001b[0mx_input_grad_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input_2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mx_input_2_grad_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input_3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-14e1fac59342>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    992\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train with the grid\n",
    "从初始化模型开始训练\n",
    "\"\"\"\n",
    "\n",
    "model = DeepRitzNet(m)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[300, 400], gamma=0.1)\n",
    "in_error_iter = [] #record the error in Omega every print_every_iter=100 times\n",
    "on_error_iter = [] #record the error on the border of Omega every print_every_iter=100 times\n",
    "\n",
    "mm = 1\n",
    "points = np.arange(-1, 1, 0.1)\n",
    "xs, ys = np.meshgrid(points, points)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xl, yl = xs.size()\n",
    "                \n",
    "for k in range(iterations):\n",
    "    n1 = 0\n",
    "    loss = torch.zeros(1)\n",
    "    for i in range(xl):\n",
    "        for j in range(yl):        \n",
    "            x_input = np.zeros(m)\n",
    "            x_input[0] = xs[i, j]\n",
    "            x_input[1] = ys[i, j]\n",
    "            if x_input[0] ** 2 + x_input[1] ** 2 < 1:\n",
    "                n1 += 1\n",
    "                x_input = torch.tensor(x_input).float()\n",
    "                y = model(x_input)\n",
    "                \n",
    "                x1 = torch.zeros(m)\n",
    "                x2 = torch.zeros(m)\n",
    "                x1[0] = 0.0001\n",
    "                x2[1] = 0.0001\n",
    "                x_input_1 = x_input.float() + x1\n",
    "                x_input_2 = x_input.float() + x2\n",
    "                x_input_3 = x_input.float() - x1\n",
    "                x_input_4 = x_input.float() - x2\n",
    "                x_input_grad_1 = (model(x_input_1) - y) / 0.0001\n",
    "                x_input_grad_2 = (model(x_input_2) - y) / 0.0001\n",
    "                x_input_2_grad_x = (model(x_input_1) + model(x_input_3) - 2 * y) / 0.0001**2\n",
    "                x_input_2_grad_y = (model(x_input_2) + model(x_input_4) - 2 * y) / 0.0001**2\n",
    "\n",
    "                loss += 0.5 * ((x_input_grad_1) ** 2 + (x_input_grad_2) ** 2) - y #+ gamma * (x_input_2_grad_x + x_input_2_grad_y) ** 2\n",
    "                #loss += 0.5 * ((x_input.grad.float()[0]) ** 2 + (x_input.grad.float()[1]) ** 2) + y\n",
    "                #loss = gamma * (x_input_2_grad_x + x_input_2_grad_y) ** 2\n",
    "    loss /= n1\n",
    "    \n",
    "    regularization = torch.zeros(1)\n",
    "    for t in range(n2):\n",
    "        theta = t / n2 * (2 * pi)\n",
    "        x_input = np.zeros(m)\n",
    "        x_input[0] = cos(theta)\n",
    "        x_input[1] = sin(theta)\n",
    "        x_input = torch.tensor(x_input).float()\n",
    "        y = model(x_input)\n",
    "        regularization += y**2 \n",
    "    regularization *= mm / n2\n",
    "    if gamma < 500:\n",
    "        gamma = gamma * 1.01\n",
    "    if mm < 500:\n",
    "        mm = mm * 1.01\n",
    "        \n",
    "    #print loss\n",
    "    print(k, \" epoch, loss: \", loss.data[0].numpy())\n",
    "    print(k, \" epoch, regularization loss: \", regularization.data[0].numpy())\n",
    "    print(k, \" loss to real solution: \", cal_loss(model))\n",
    "    if cal_loss(model) < 0.0001:\n",
    "        break\n",
    "    \n",
    "    loss += regularization\n",
    "    \n",
    "    #and step the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #scheduler.step()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAD8CAYAAADnhGhBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nugkTJhx5/P+Q4a2I5W1QHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012661244662244994\n"
     ]
    }
   ],
   "source": [
    "validate(the_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013054753003282139"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_err(the_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only save the parameters\n",
    "PATH = 'test_parameters.pkl'\n",
    "torch.save(the_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "PATH = 'test_parameters.pkl'\n",
    "#从文件读取the_model\n",
    "the_model = DeepRitzNet(m)\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  epoch, loss:  -0.088172175\n",
      "0  epoch, regularization loss:  0.0012239327\n",
      "0  loss to real solution:  0.013472990111906032\n",
      "1  epoch, loss:  -0.0882291\n",
      "1  epoch, regularization loss:  0.001237815\n",
      "1  loss to real solution:  0.013481815671997424\n",
      "2  epoch, loss:  -0.08824132\n",
      "2  epoch, regularization loss:  0.0012518794\n",
      "2  loss to real solution:  0.01349083595337207\n",
      "3  epoch, loss:  -0.08827846\n",
      "3  epoch, regularization loss:  0.0012661172\n",
      "3  loss to real solution:  0.013499944779267251\n",
      "4  epoch, loss:  -0.088293396\n",
      "4  epoch, regularization loss:  0.0012805214\n",
      "4  loss to real solution:  0.01350929377929956\n",
      "5  epoch, loss:  -0.0883347\n",
      "5  epoch, regularization loss:  0.0012950946\n",
      "5  loss to real solution:  0.01351871069029595\n",
      "6  epoch, loss:  -0.08835273\n",
      "6  epoch, regularization loss:  0.001309836\n",
      "6  loss to real solution:  0.013528317845519318\n",
      "7  epoch, loss:  -0.08838569\n",
      "7  epoch, regularization loss:  0.0013247484\n",
      "7  loss to real solution:  0.013538012560157518\n",
      "8  epoch, loss:  -0.08839417\n",
      "8  epoch, regularization loss:  0.0013398316\n",
      "8  loss to real solution:  0.01354771762415526\n",
      "9  epoch, loss:  -0.088412374\n",
      "9  epoch, regularization loss:  0.0013550034\n",
      "9  loss to real solution:  0.013557093425195689\n",
      "10  epoch, loss:  -0.08847301\n",
      "10  epoch, regularization loss:  0.0013703475\n",
      "10  loss to real solution:  0.01356655932699368\n",
      "11  epoch, loss:  -0.08849465\n",
      "11  epoch, regularization loss:  0.0013858579\n",
      "11  loss to real solution:  0.013576035842634844\n",
      "12  epoch, loss:  -0.08852268\n",
      "12  epoch, regularization loss:  0.0014015379\n",
      "12  loss to real solution:  0.013585464444574423\n",
      "13  epoch, loss:  -0.08853148\n",
      "13  epoch, regularization loss:  0.0014173917\n",
      "13  loss to real solution:  0.013594852799004667\n",
      "14  epoch, loss:  -0.08858062\n",
      "14  epoch, regularization loss:  0.0014334162\n",
      "14  loss to real solution:  0.013604198797722707\n",
      "15  epoch, loss:  -0.0886284\n",
      "15  epoch, regularization loss:  0.0014496162\n",
      "15  loss to real solution:  0.013613646745298441\n",
      "16  epoch, loss:  -0.08862328\n",
      "16  epoch, regularization loss:  0.0014659883\n",
      "16  loss to real solution:  0.013623166287443634\n",
      "17  epoch, loss:  -0.08864809\n",
      "17  epoch, regularization loss:  0.001482542\n",
      "17  loss to real solution:  0.013632845317052479\n",
      "18  epoch, loss:  -0.08867889\n",
      "18  epoch, regularization loss:  0.001499278\n",
      "18  loss to real solution:  0.013642663541714083\n",
      "19  epoch, loss:  -0.08868996\n",
      "19  epoch, regularization loss:  0.0015162033\n",
      "19  loss to real solution:  0.013652508034583441\n",
      "20  epoch, loss:  -0.08871505\n",
      "20  epoch, regularization loss:  0.0015333305\n",
      "20  loss to real solution:  0.013662441263628144\n",
      "21  epoch, loss:  -0.08873654\n",
      "21  epoch, regularization loss:  0.0015506593\n",
      "21  loss to real solution:  0.013672442721783907\n",
      "22  epoch, loss:  -0.088771746\n",
      "22  epoch, regularization loss:  0.0015681752\n",
      "22  loss to real solution:  0.013682415623373525\n",
      "23  epoch, loss:  -0.08877487\n",
      "23  epoch, regularization loss:  0.0015858863\n",
      "23  loss to real solution:  0.01369245162155849\n",
      "24  epoch, loss:  -0.08883197\n",
      "24  epoch, regularization loss:  0.0016037873\n",
      "24  loss to real solution:  0.013702542103371804\n",
      "25  epoch, loss:  -0.08883943\n",
      "25  epoch, regularization loss:  0.0016218855\n",
      "25  loss to real solution:  0.013712605561857424\n",
      "26  epoch, loss:  -0.088869326\n",
      "26  epoch, regularization loss:  0.0016401787\n",
      "26  loss to real solution:  0.013722875992201516\n",
      "27  epoch, loss:  -0.08887996\n",
      "27  epoch, regularization loss:  0.0016586754\n",
      "27  loss to real solution:  0.01373321064223812\n",
      "28  epoch, loss:  -0.088919364\n",
      "28  epoch, regularization loss:  0.0016773749\n",
      "28  loss to real solution:  0.013743554783020737\n",
      "29  epoch, loss:  -0.08895152\n",
      "29  epoch, regularization loss:  0.0016961904\n",
      "29  loss to real solution:  0.013753739175689164\n",
      "30  epoch, loss:  -0.08895451\n",
      "30  epoch, regularization loss:  0.0017152221\n",
      "30  loss to real solution:  0.013763941008944968\n",
      "31  epoch, loss:  -0.08901417\n",
      "31  epoch, regularization loss:  0.0017344719\n",
      "31  loss to real solution:  0.013774157791275667\n",
      "32  epoch, loss:  -0.0890046\n",
      "32  epoch, regularization loss:  0.0017539649\n",
      "32  loss to real solution:  0.013784551854302243\n",
      "33  epoch, loss:  -0.089050815\n",
      "33  epoch, regularization loss:  0.0017736943\n",
      "33  loss to real solution:  0.013795047302721379\n",
      "34  epoch, loss:  -0.08905346\n",
      "34  epoch, regularization loss:  0.0017936454\n",
      "34  loss to real solution:  0.013805533935019421\n",
      "35  epoch, loss:  -0.08909521\n",
      "35  epoch, regularization loss:  0.0018138154\n",
      "35  loss to real solution:  0.013816141076026604\n",
      "36  epoch, loss:  -0.08913203\n",
      "36  epoch, regularization loss:  0.0018342086\n",
      "36  loss to real solution:  0.013826733022640739\n",
      "37  epoch, loss:  -0.08916815\n",
      "37  epoch, regularization loss:  0.0018548304\n",
      "37  loss to real solution:  0.013837321902777972\n",
      "38  epoch, loss:  -0.08915682\n",
      "38  epoch, regularization loss:  0.0018756754\n",
      "38  loss to real solution:  0.013847920773880258\n",
      "39  epoch, loss:  -0.08919298\n",
      "39  epoch, regularization loss:  0.0018967439\n",
      "39  loss to real solution:  0.013858593742563777\n",
      "40  epoch, loss:  -0.08921167\n",
      "40  epoch, regularization loss:  0.001918051\n",
      "40  loss to real solution:  0.013869275144058776\n",
      "41  epoch, loss:  -0.089263625\n",
      "41  epoch, regularization loss:  0.0019395917\n",
      "41  loss to real solution:  0.013879988540407145\n",
      "42  epoch, loss:  -0.08927857\n",
      "42  epoch, regularization loss:  0.0019613025\n",
      "42  loss to real solution:  0.013890512018341709\n",
      "43  epoch, loss:  -0.089293085\n",
      "43  epoch, regularization loss:  0.0019832905\n",
      "43  loss to real solution:  0.013901221784748048\n",
      "44  epoch, loss:  -0.08934749\n",
      "44  epoch, regularization loss:  0.0020055224\n",
      "44  loss to real solution:  0.013911975037629948\n",
      "45  epoch, loss:  -0.08935832\n",
      "45  epoch, regularization loss:  0.0020280068\n",
      "45  loss to real solution:  0.013922823562116085\n",
      "46  epoch, loss:  -0.08935242\n",
      "46  epoch, regularization loss:  0.0020507686\n",
      "46  loss to real solution:  0.013933830201817463\n",
      "47  epoch, loss:  -0.08939013\n",
      "47  epoch, regularization loss:  0.0020737818\n",
      "47  loss to real solution:  0.013944851733097294\n",
      "48  epoch, loss:  -0.08941413\n",
      "48  epoch, regularization loss:  0.0020970488\n",
      "48  loss to real solution:  0.013955943467532692\n",
      "49  epoch, loss:  -0.08946049\n",
      "49  epoch, regularization loss:  0.0021205712\n",
      "49  loss to real solution:  0.01396700779533078\n",
      "50  epoch, loss:  -0.08945378\n",
      "50  epoch, regularization loss:  0.0021443577\n",
      "50  loss to real solution:  0.013978062732043358\n",
      "51  epoch, loss:  -0.08950608\n",
      "51  epoch, regularization loss:  0.0021684035\n",
      "51  loss to real solution:  0.01398909217866669\n",
      "52  epoch, loss:  -0.08951696\n",
      "52  epoch, regularization loss:  0.0021927177\n",
      "52  loss to real solution:  0.014000115875645834\n",
      "53  epoch, loss:  -0.08953968\n",
      "53  epoch, regularization loss:  0.0022173\n",
      "53  loss to real solution:  0.014011114274190538\n",
      "54  epoch, loss:  -0.08958451\n",
      "54  epoch, regularization loss:  0.0022421533\n",
      "54  loss to real solution:  0.014022098490279572\n",
      "55  epoch, loss:  -0.089578964\n",
      "55  epoch, regularization loss:  0.0022672894\n",
      "55  loss to real solution:  0.014033099571991564\n",
      "56  epoch, loss:  -0.089626715\n",
      "56  epoch, regularization loss:  0.0022927346\n",
      "56  loss to real solution:  0.014044271820611109\n",
      "57  epoch, loss:  -0.089622796\n",
      "57  epoch, regularization loss:  0.0023184447\n",
      "57  loss to real solution:  0.014055602356935221\n",
      "58  epoch, loss:  -0.08966135\n",
      "58  epoch, regularization loss:  0.0023443906\n",
      "58  loss to real solution:  0.014066792410286277\n",
      "59  epoch, loss:  -0.08966793\n",
      "59  epoch, regularization loss:  0.002370519\n",
      "59  loss to real solution:  0.014077589187974698\n",
      "60  epoch, loss:  -0.08970675\n",
      "60  epoch, regularization loss:  0.0023969486\n",
      "60  loss to real solution:  0.014088406856037013\n",
      "61  epoch, loss:  -0.08975424\n",
      "61  epoch, regularization loss:  0.0024236676\n",
      "61  loss to real solution:  0.01409918810968612\n",
      "62  epoch, loss:  -0.0897698\n",
      "62  epoch, regularization loss:  0.0024506887\n",
      "62  loss to real solution:  0.014109995620043697\n",
      "63  epoch, loss:  -0.08978904\n",
      "63  epoch, regularization loss:  0.002478016\n",
      "63  loss to real solution:  0.014120781090098546\n",
      "64  epoch, loss:  -0.08979435\n",
      "64  epoch, regularization loss:  0.002505654\n",
      "64  loss to real solution:  0.014131582898725634\n",
      "65  epoch, loss:  -0.08980562\n",
      "65  epoch, regularization loss:  0.0025335993\n",
      "65  loss to real solution:  0.014142382407495047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66  epoch, loss:  -0.089841425\n",
      "66  epoch, regularization loss:  0.002561862\n",
      "66  loss to real solution:  0.01415318651597981\n",
      "67  epoch, loss:  -0.089886144\n",
      "67  epoch, regularization loss:  0.0025904805\n",
      "67  loss to real solution:  0.014164173846459446\n",
      "68  epoch, loss:  -0.08987862\n",
      "68  epoch, regularization loss:  0.0026194595\n",
      "68  loss to real solution:  0.014175319675463935\n",
      "69  epoch, loss:  -0.08990314\n",
      "69  epoch, regularization loss:  0.0026487522\n",
      "69  loss to real solution:  0.014186431006603283\n",
      "70  epoch, loss:  -0.08993975\n",
      "70  epoch, regularization loss:  0.0026783654\n",
      "70  loss to real solution:  0.014197498832101588\n",
      "71  epoch, loss:  -0.08995296\n",
      "71  epoch, regularization loss:  0.0027083005\n",
      "71  loss to real solution:  0.014208538292688552\n",
      "72  epoch, loss:  -0.089969486\n",
      "72  epoch, regularization loss:  0.0027385638\n",
      "72  loss to real solution:  0.014219536739146959\n",
      "73  epoch, loss:  -0.0900147\n",
      "73  epoch, regularization loss:  0.0027691573\n",
      "73  loss to real solution:  0.014230502604288283\n",
      "74  epoch, loss:  -0.09004151\n",
      "74  epoch, regularization loss:  0.0028000744\n",
      "74  loss to real solution:  0.014241398707080098\n",
      "75  epoch, loss:  -0.09003914\n",
      "75  epoch, regularization loss:  0.0028313226\n",
      "75  loss to real solution:  0.01425229476770785\n",
      "76  epoch, loss:  -0.09006994\n",
      "76  epoch, regularization loss:  0.0028629145\n",
      "76  loss to real solution:  0.01426328966855234\n",
      "77  epoch, loss:  -0.090116955\n",
      "77  epoch, regularization loss:  0.0028948553\n",
      "77  loss to real solution:  0.014274388020828201\n",
      "78  epoch, loss:  -0.09011423\n",
      "78  epoch, regularization loss:  0.0029271473\n",
      "78  loss to real solution:  0.014285508551398252\n",
      "79  epoch, loss:  -0.090152085\n",
      "79  epoch, regularization loss:  0.002959799\n",
      "79  loss to real solution:  0.014296641206051355\n",
      "80  epoch, loss:  -0.09016306\n",
      "80  epoch, regularization loss:  0.002992861\n",
      "80  loss to real solution:  0.014308101444765665\n",
      "81  epoch, loss:  -0.090168945\n",
      "81  epoch, regularization loss:  0.0030262412\n",
      "81  loss to real solution:  0.01431944688225097\n",
      "82  epoch, loss:  -0.09022995\n",
      "82  epoch, regularization loss:  0.0030599863\n",
      "82  loss to real solution:  0.01433076409856604\n",
      "83  epoch, loss:  -0.090238586\n",
      "83  epoch, regularization loss:  0.003094096\n",
      "83  loss to real solution:  0.014342029568083395\n",
      "84  epoch, loss:  -0.09024176\n",
      "84  epoch, regularization loss:  0.0031285817\n",
      "84  loss to real solution:  0.014353262072974058\n",
      "85  epoch, loss:  -0.0902796\n",
      "85  epoch, regularization loss:  0.0031634485\n",
      "85  loss to real solution:  0.014364500902473331\n",
      "86  epoch, loss:  -0.09029888\n",
      "86  epoch, regularization loss:  0.0031986956\n",
      "86  loss to real solution:  0.01437571270864491\n",
      "87  epoch, loss:  -0.090329275\n",
      "87  epoch, regularization loss:  0.0032342789\n",
      "87  loss to real solution:  0.01438679936422794\n",
      "88  epoch, loss:  -0.09035827\n",
      "88  epoch, regularization loss:  0.003270208\n",
      "88  loss to real solution:  0.014397853940629483\n",
      "89  epoch, loss:  -0.090409905\n",
      "89  epoch, regularization loss:  0.0033065397\n",
      "89  loss to real solution:  0.014408960719967175\n",
      "90  epoch, loss:  -0.090398595\n",
      "90  epoch, regularization loss:  0.0033433316\n",
      "90  loss to real solution:  0.014420243630071909\n",
      "91  epoch, loss:  -0.09042646\n",
      "91  epoch, regularization loss:  0.0033805347\n",
      "91  loss to real solution:  0.01443153497298812\n",
      "92  epoch, loss:  -0.09044125\n",
      "92  epoch, regularization loss:  0.0034181494\n",
      "92  loss to real solution:  0.014442848873675041\n",
      "93  epoch, loss:  -0.09048548\n",
      "93  epoch, regularization loss:  0.0034561744\n",
      "93  loss to real solution:  0.01445422550298008\n",
      "94  epoch, loss:  -0.090486825\n",
      "94  epoch, regularization loss:  0.0034946247\n",
      "94  loss to real solution:  0.014465590249687124\n",
      "95  epoch, loss:  -0.090543374\n",
      "95  epoch, regularization loss:  0.0035334928\n",
      "95  loss to real solution:  0.014476936789187565\n",
      "96  epoch, loss:  -0.09051899\n",
      "96  epoch, regularization loss:  0.0035728526\n",
      "96  loss to real solution:  0.014488467844352822\n",
      "97  epoch, loss:  -0.09059406\n",
      "97  epoch, regularization loss:  0.0036126333\n",
      "97  loss to real solution:  0.014499953668983781\n",
      "98  epoch, loss:  -0.09058917\n",
      "98  epoch, regularization loss:  0.0036528443\n",
      "98  loss to real solution:  0.01451139426308044\n",
      "99  epoch, loss:  -0.090623766\n",
      "99  epoch, regularization loss:  0.0036933848\n",
      "99  loss to real solution:  0.01452252801783213\n",
      "100  epoch, loss:  -0.09064531\n",
      "100  epoch, regularization loss:  0.0037343802\n",
      "100  loss to real solution:  0.014533691594071687\n",
      "101  epoch, loss:  -0.090676345\n",
      "101  epoch, regularization loss:  0.0037758236\n",
      "101  loss to real solution:  0.014544977043602605\n",
      "102  epoch, loss:  -0.090679534\n",
      "102  epoch, regularization loss:  0.0038177287\n",
      "102  loss to real solution:  0.014556252718738397\n",
      "103  epoch, loss:  -0.09071616\n",
      "103  epoch, regularization loss:  0.0038601474\n",
      "103  loss to real solution:  0.014567713532417127\n",
      "104  epoch, loss:  -0.09071278\n",
      "104  epoch, regularization loss:  0.0039030854\n",
      "104  loss to real solution:  0.014579401158059885\n",
      "105  epoch, loss:  -0.09075349\n",
      "105  epoch, regularization loss:  0.003946481\n",
      "105  loss to real solution:  0.014591223509365316\n",
      "106  epoch, loss:  -0.09079614\n",
      "106  epoch, regularization loss:  0.003990345\n",
      "106  loss to real solution:  0.014603017419097485\n",
      "107  epoch, loss:  -0.09078888\n",
      "107  epoch, regularization loss:  0.004034606\n",
      "107  loss to real solution:  0.014614603191709956\n",
      "108  epoch, loss:  -0.090820365\n",
      "108  epoch, regularization loss:  0.0040792883\n",
      "108  loss to real solution:  0.01462604392954772\n",
      "109  epoch, loss:  -0.090835966\n",
      "109  epoch, regularization loss:  0.0041244538\n",
      "109  loss to real solution:  0.014637448827936696\n",
      "110  epoch, loss:  -0.09085506\n",
      "110  epoch, regularization loss:  0.004170115\n",
      "110  loss to real solution:  0.014648818461841302\n",
      "111  epoch, loss:  -0.09087044\n",
      "111  epoch, regularization loss:  0.0042162687\n",
      "111  loss to real solution:  0.014660156472702862\n",
      "112  epoch, loss:  -0.09088383\n",
      "112  epoch, regularization loss:  0.004262935\n",
      "112  loss to real solution:  0.014671491225432714\n",
      "113  epoch, loss:  -0.090948805\n",
      "113  epoch, regularization loss:  0.0043101115\n",
      "113  loss to real solution:  0.014682795505048354\n",
      "114  epoch, loss:  -0.09096172\n",
      "114  epoch, regularization loss:  0.0043578157\n",
      "114  loss to real solution:  0.014694106492582218\n",
      "115  epoch, loss:  -0.09098253\n",
      "115  epoch, regularization loss:  0.004406044\n",
      "115  loss to real solution:  0.01470541709680647\n",
      "116  epoch, loss:  -0.09097499\n",
      "116  epoch, regularization loss:  0.0044548875\n",
      "116  loss to real solution:  0.014716948295712831\n",
      "117  epoch, loss:  -0.09100276\n",
      "117  epoch, regularization loss:  0.0045043407\n",
      "117  loss to real solution:  0.014728680540511057\n",
      "118  epoch, loss:  -0.0910373\n",
      "118  epoch, regularization loss:  0.004554318\n",
      "118  loss to real solution:  0.014740339573173255\n",
      "119  epoch, loss:  -0.091047674\n",
      "119  epoch, regularization loss:  0.0046048225\n",
      "119  loss to real solution:  0.014751935216008252\n",
      "120  epoch, loss:  -0.09108886\n",
      "120  epoch, regularization loss:  0.0046557346\n",
      "120  loss to real solution:  0.01476317840565437\n",
      "121  epoch, loss:  -0.09111303\n",
      "121  epoch, regularization loss:  0.0047072\n",
      "121  loss to real solution:  0.014774398021759306\n",
      "122  epoch, loss:  -0.0911449\n",
      "122  epoch, regularization loss:  0.0047592255\n",
      "122  loss to real solution:  0.01478558505654716\n",
      "123  epoch, loss:  -0.091131836\n",
      "123  epoch, regularization loss:  0.004811816\n",
      "123  loss to real solution:  0.014796781881062527\n",
      "124  epoch, loss:  -0.09116809\n",
      "124  epoch, regularization loss:  0.0048649576\n",
      "124  loss to real solution:  0.014808041073884974\n",
      "125  epoch, loss:  -0.09119144\n",
      "125  epoch, regularization loss:  0.004918675\n",
      "125  loss to real solution:  0.014819268068699954\n",
      "126  epoch, loss:  -0.09122538\n",
      "126  epoch, regularization loss:  0.0049730637\n",
      "126  loss to real solution:  0.014830699367538506\n",
      "127  epoch, loss:  -0.09122976\n",
      "127  epoch, regularization loss:  0.0050280388\n",
      "127  loss to real solution:  0.014842173275073603\n",
      "128  epoch, loss:  -0.091256246\n",
      "128  epoch, regularization loss:  0.00508359\n",
      "128  loss to real solution:  0.014853656464450974\n",
      "129  epoch, loss:  -0.09125279\n",
      "129  epoch, regularization loss:  0.0051396675\n",
      "129  loss to real solution:  0.014864999554164902\n",
      "130  epoch, loss:  -0.09129915\n",
      "130  epoch, regularization loss:  0.0051963474\n",
      "130  loss to real solution:  0.014876309870906943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131  epoch, loss:  -0.091312595\n",
      "131  epoch, regularization loss:  0.0052537504\n",
      "131  loss to real solution:  0.014887844615427222\n",
      "132  epoch, loss:  -0.09134358\n",
      "132  epoch, regularization loss:  0.0053117494\n",
      "132  loss to real solution:  0.014899311897455665\n",
      "133  epoch, loss:  -0.09136387\n",
      "133  epoch, regularization loss:  0.0053701857\n",
      "133  loss to real solution:  0.014910336449215241\n",
      "134  epoch, loss:  -0.09137352\n",
      "134  epoch, regularization loss:  0.005429092\n",
      "134  loss to real solution:  0.01492106950168055\n",
      "135  epoch, loss:  -0.091394566\n",
      "135  epoch, regularization loss:  0.00548852\n",
      "135  loss to real solution:  0.014931525421295876\n",
      "136  epoch, loss:  -0.09141934\n",
      "136  epoch, regularization loss:  0.0055485237\n",
      "136  loss to real solution:  0.014941959578507944\n",
      "137  epoch, loss:  -0.09144768\n",
      "137  epoch, regularization loss:  0.005609212\n",
      "137  loss to real solution:  0.014952502557319055\n",
      "138  epoch, loss:  -0.09146719\n",
      "138  epoch, regularization loss:  0.0056705847\n",
      "138  loss to real solution:  0.014963084825365467\n",
      "139  epoch, loss:  -0.09148383\n",
      "139  epoch, regularization loss:  0.0057327636\n",
      "139  loss to real solution:  0.014973974699376075\n",
      "140  epoch, loss:  -0.0915129\n",
      "140  epoch, regularization loss:  0.005795595\n",
      "140  loss to real solution:  0.014984819917816802\n",
      "141  epoch, loss:  -0.09150827\n",
      "141  epoch, regularization loss:  0.0058591277\n",
      "141  loss to real solution:  0.014995719389900108\n",
      "142  epoch, loss:  -0.09152365\n",
      "142  epoch, regularization loss:  0.0059233503\n",
      "142  loss to real solution:  0.015006694837781751\n",
      "143  epoch, loss:  -0.0915272\n",
      "143  epoch, regularization loss:  0.005988264\n",
      "143  loss to real solution:  0.015017659340257408\n",
      "144  epoch, loss:  -0.09159485\n",
      "144  epoch, regularization loss:  0.0060538966\n",
      "144  loss to real solution:  0.01502873612561791\n",
      "145  epoch, loss:  -0.09159559\n",
      "145  epoch, regularization loss:  0.0061201975\n",
      "145  loss to real solution:  0.015039759822597051\n",
      "146  epoch, loss:  -0.09159371\n",
      "146  epoch, regularization loss:  0.006187228\n",
      "146  loss to real solution:  0.015050785052814646\n",
      "147  epoch, loss:  -0.091638096\n",
      "147  epoch, regularization loss:  0.0062549827\n",
      "147  loss to real solution:  0.015061802041875565\n",
      "148  epoch, loss:  -0.091676384\n",
      "148  epoch, regularization loss:  0.0063234833\n",
      "148  loss to real solution:  0.015072898333862259\n",
      "149  epoch, loss:  -0.091654696\n",
      "149  epoch, regularization loss:  0.0063927188\n",
      "149  loss to real solution:  0.015084009760829008\n",
      "150  epoch, loss:  -0.09165658\n",
      "150  epoch, regularization loss:  0.0064628264\n",
      "150  loss to real solution:  0.01509537532206898\n",
      "151  epoch, loss:  -0.09172067\n",
      "151  epoch, regularization loss:  0.006533419\n",
      "151  loss to real solution:  0.01510631502632924\n",
      "152  epoch, loss:  -0.09176261\n",
      "152  epoch, regularization loss:  0.0066047716\n",
      "152  loss to real solution:  0.015117301398534844\n",
      "153  epoch, loss:  -0.091756605\n",
      "153  epoch, regularization loss:  0.0066768806\n",
      "153  loss to real solution:  0.015128232382501414\n",
      "154  epoch, loss:  -0.09178615\n",
      "154  epoch, regularization loss:  0.006749602\n",
      "154  loss to real solution:  0.015138804971980106\n",
      "155  epoch, loss:  -0.09182038\n",
      "155  epoch, regularization loss:  0.006823126\n",
      "155  loss to real solution:  0.015149518042515303\n",
      "156  epoch, loss:  -0.09181341\n",
      "156  epoch, regularization loss:  0.006897461\n",
      "156  loss to real solution:  0.015160293642347623\n",
      "157  epoch, loss:  -0.091855645\n",
      "157  epoch, regularization loss:  0.00697267\n",
      "157  loss to real solution:  0.015171144945828451\n",
      "158  epoch, loss:  -0.09187471\n",
      "158  epoch, regularization loss:  0.007048829\n",
      "158  loss to real solution:  0.015182271082301588\n",
      "159  epoch, loss:  -0.0918958\n",
      "159  epoch, regularization loss:  0.0071258075\n",
      "159  loss to real solution:  0.015193360421051906\n",
      "160  epoch, loss:  -0.09189306\n",
      "160  epoch, regularization loss:  0.0072036125\n",
      "160  loss to real solution:  0.01520445895923293\n",
      "161  epoch, loss:  -0.09191386\n",
      "161  epoch, regularization loss:  0.007282079\n",
      "161  loss to real solution:  0.015215171840029872\n",
      "162  epoch, loss:  -0.0919277\n",
      "162  epoch, regularization loss:  0.007361397\n",
      "162  loss to real solution:  0.01522599705737501\n",
      "163  epoch, loss:  -0.09192923\n",
      "163  epoch, regularization loss:  0.0074415565\n",
      "163  loss to real solution:  0.015236901290163708\n",
      "164  epoch, loss:  -0.091944866\n",
      "164  epoch, regularization loss:  0.0075225835\n",
      "164  loss to real solution:  0.015247936534344933\n",
      "165  epoch, loss:  -0.09198547\n",
      "165  epoch, regularization loss:  0.007604471\n",
      "165  loss to real solution:  0.015258916390287121\n",
      "166  epoch, loss:  -0.09199454\n",
      "166  epoch, regularization loss:  0.007687226\n",
      "166  loss to real solution:  0.015269851159436118\n",
      "167  epoch, loss:  -0.092049815\n",
      "167  epoch, regularization loss:  0.00777086\n",
      "167  loss to real solution:  0.01528075928856703\n",
      "168  epoch, loss:  -0.09203806\n",
      "168  epoch, regularization loss:  0.007855206\n",
      "168  loss to real solution:  0.015291317456021544\n",
      "169  epoch, loss:  -0.09205315\n",
      "169  epoch, regularization loss:  0.007940452\n",
      "169  loss to real solution:  0.015301856266340617\n",
      "170  epoch, loss:  -0.09209644\n",
      "170  epoch, regularization loss:  0.008026637\n",
      "170  loss to real solution:  0.015312406192638458\n",
      "171  epoch, loss:  -0.09210122\n",
      "171  epoch, regularization loss:  0.008113893\n",
      "171  loss to real solution:  0.015323227837538004\n",
      "172  epoch, loss:  -0.09211842\n",
      "172  epoch, regularization loss:  0.008201847\n",
      "172  loss to real solution:  0.01533365697339414\n",
      "173  epoch, loss:  -0.09213652\n",
      "173  epoch, regularization loss:  0.008290726\n",
      "173  loss to real solution:  0.015344035512381402\n",
      "174  epoch, loss:  -0.09216271\n",
      "174  epoch, regularization loss:  0.008380528\n",
      "174  loss to real solution:  0.015354352338521018\n",
      "175  epoch, loss:  -0.09217523\n",
      "175  epoch, regularization loss:  0.008471453\n",
      "175  loss to real solution:  0.015364947974490173\n",
      "176  epoch, loss:  -0.092213266\n",
      "176  epoch, regularization loss:  0.008563331\n",
      "176  loss to real solution:  0.015375503470276688\n",
      "177  epoch, loss:  -0.092207566\n",
      "177  epoch, regularization loss:  0.008656171\n",
      "177  loss to real solution:  0.015386089523505526\n",
      "178  epoch, loss:  -0.09222127\n",
      "178  epoch, regularization loss:  0.008749656\n",
      "178  loss to real solution:  0.015396168458116758\n",
      "179  epoch, loss:  -0.09221912\n",
      "179  epoch, regularization loss:  0.008844123\n",
      "179  loss to real solution:  0.015406305224565806\n",
      "180  epoch, loss:  -0.09225446\n",
      "180  epoch, regularization loss:  0.008939625\n",
      "180  loss to real solution:  0.01541647533895116\n",
      "181  epoch, loss:  -0.09226991\n",
      "181  epoch, regularization loss:  0.009036145\n",
      "181  loss to real solution:  0.015426621113176105\n",
      "182  epoch, loss:  -0.092291154\n",
      "182  epoch, regularization loss:  0.009133645\n",
      "182  loss to real solution:  0.015436669143449813\n",
      "183  epoch, loss:  -0.092291\n",
      "183  epoch, regularization loss:  0.009232354\n",
      "183  loss to real solution:  0.015446983142681035\n",
      "184  epoch, loss:  -0.09233973\n",
      "184  epoch, regularization loss:  0.009332278\n",
      "184  loss to real solution:  0.015457536518765391\n",
      "185  epoch, loss:  -0.092345804\n",
      "185  epoch, regularization loss:  0.009433215\n",
      "185  loss to real solution:  0.015467981226574544\n",
      "186  epoch, loss:  -0.09235627\n",
      "186  epoch, regularization loss:  0.009534969\n",
      "186  loss to real solution:  0.015478035725197965\n",
      "187  epoch, loss:  -0.09236561\n",
      "187  epoch, regularization loss:  0.009637769\n",
      "187  loss to real solution:  0.015488051143489825\n",
      "188  epoch, loss:  -0.09240214\n",
      "188  epoch, regularization loss:  0.009741628\n",
      "188  loss to real solution:  0.015498081733176154\n",
      "189  epoch, loss:  -0.0924092\n",
      "189  epoch, regularization loss:  0.00984652\n",
      "189  loss to real solution:  0.015508002121348832\n",
      "190  epoch, loss:  -0.09242206\n",
      "190  epoch, regularization loss:  0.009952489\n",
      "190  loss to real solution:  0.015517836648168264\n",
      "191  epoch, loss:  -0.09242026\n",
      "191  epoch, regularization loss:  0.010059536\n",
      "191  loss to real solution:  0.015527602993790334\n",
      "192  epoch, loss:  -0.09245027\n",
      "192  epoch, regularization loss:  0.010167547\n",
      "192  loss to real solution:  0.015537158902435034\n",
      "193  epoch, loss:  -0.09248012\n",
      "193  epoch, regularization loss:  0.010276686\n",
      "193  loss to real solution:  0.015546744440912792\n",
      "194  epoch, loss:  -0.09247147\n",
      "194  epoch, regularization loss:  0.01038695\n",
      "194  loss to real solution:  0.015556300876608206\n",
      "195  epoch, loss:  -0.09250144\n",
      "195  epoch, regularization loss:  0.010498356\n",
      "195  loss to real solution:  0.01556584581301524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196  epoch, loss:  -0.09253243\n",
      "196  epoch, regularization loss:  0.010611118\n",
      "196  loss to real solution:  0.015575619776915866\n",
      "197  epoch, loss:  -0.092521876\n",
      "197  epoch, regularization loss:  0.010724984\n",
      "197  loss to real solution:  0.015585255701442214\n",
      "198  epoch, loss:  -0.09252956\n",
      "198  epoch, regularization loss:  0.010840006\n",
      "198  loss to real solution:  0.015594809406056639\n",
      "199  epoch, loss:  -0.09257665\n",
      "199  epoch, regularization loss:  0.010956205\n",
      "199  loss to real solution:  0.015604286257093716\n",
      "200  epoch, loss:  -0.09257309\n",
      "200  epoch, regularization loss:  0.011073591\n",
      "200  loss to real solution:  0.01561369598103487\n",
      "201  epoch, loss:  -0.09259177\n",
      "201  epoch, regularization loss:  0.0111921625\n",
      "201  loss to real solution:  0.015623017160455489\n",
      "202  epoch, loss:  -0.09259643\n",
      "202  epoch, regularization loss:  0.011311888\n",
      "202  loss to real solution:  0.0156321842494118\n",
      "203  epoch, loss:  -0.09259872\n",
      "203  epoch, regularization loss:  0.011432793\n",
      "203  loss to real solution:  0.015641246455275336\n",
      "204  epoch, loss:  -0.0926534\n",
      "204  epoch, regularization loss:  0.011554628\n",
      "204  loss to real solution:  0.015649873988038066\n",
      "205  epoch, loss:  -0.09263838\n",
      "205  epoch, regularization loss:  0.011677687\n",
      "205  loss to real solution:  0.015658452457170367\n",
      "206  epoch, loss:  -0.0926899\n",
      "206  epoch, regularization loss:  0.011802268\n",
      "206  loss to real solution:  0.015667331009815717\n",
      "207  epoch, loss:  -0.09267749\n",
      "207  epoch, regularization loss:  0.01192811\n",
      "207  loss to real solution:  0.01567612427607224\n",
      "208  epoch, loss:  -0.09270482\n",
      "208  epoch, regularization loss:  0.012055188\n",
      "208  loss to real solution:  0.01568487986682693\n",
      "209  epoch, loss:  -0.09272528\n",
      "209  epoch, regularization loss:  0.012183514\n",
      "209  loss to real solution:  0.015693523594420795\n",
      "210  epoch, loss:  -0.092739716\n",
      "210  epoch, regularization loss:  0.012312928\n",
      "210  loss to real solution:  0.01570189574715406\n",
      "211  epoch, loss:  -0.09275837\n",
      "211  epoch, regularization loss:  0.012443649\n",
      "211  loss to real solution:  0.015710200293654386\n",
      "212  epoch, loss:  -0.092772\n",
      "212  epoch, regularization loss:  0.012575684\n",
      "212  loss to real solution:  0.015718422811897594\n",
      "213  epoch, loss:  -0.09277408\n",
      "213  epoch, regularization loss:  0.01270904\n",
      "213  loss to real solution:  0.015726531870495438\n",
      "214  epoch, loss:  -0.09279247\n",
      "214  epoch, regularization loss:  0.012843716\n",
      "214  loss to real solution:  0.01573453844168559\n",
      "215  epoch, loss:  -0.0928158\n",
      "215  epoch, regularization loss:  0.012980006\n",
      "215  loss to real solution:  0.015742783239800034\n",
      "216  epoch, loss:  -0.09283221\n",
      "216  epoch, regularization loss:  0.013117363\n",
      "216  loss to real solution:  0.01575063155203384\n",
      "217  epoch, loss:  -0.092813715\n",
      "217  epoch, regularization loss:  0.013256098\n",
      "217  loss to real solution:  0.015758400135868204\n",
      "218  epoch, loss:  -0.092835724\n",
      "218  epoch, regularization loss:  0.013396164\n",
      "218  loss to real solution:  0.015766038250693143\n",
      "219  epoch, loss:  -0.09284864\n",
      "219  epoch, regularization loss:  0.013537921\n",
      "219  loss to real solution:  0.015773902134879964\n",
      "220  epoch, loss:  -0.09285721\n",
      "220  epoch, regularization loss:  0.013681044\n",
      "220  loss to real solution:  0.015781647384741655\n",
      "221  epoch, loss:  -0.09289039\n",
      "221  epoch, regularization loss:  0.013825596\n",
      "221  loss to real solution:  0.01578929053050526\n",
      "222  epoch, loss:  -0.092883684\n",
      "222  epoch, regularization loss:  0.013971536\n",
      "222  loss to real solution:  0.015796806609132255\n",
      "223  epoch, loss:  -0.09289374\n",
      "223  epoch, regularization loss:  0.014118834\n",
      "223  loss to real solution:  0.015804137597130015\n",
      "224  epoch, loss:  -0.09292639\n",
      "224  epoch, regularization loss:  0.01426755\n",
      "224  loss to real solution:  0.015811347075980548\n",
      "225  epoch, loss:  -0.09293856\n",
      "225  epoch, regularization loss:  0.014417751\n",
      "225  loss to real solution:  0.015818469974772313\n",
      "226  epoch, loss:  -0.0929404\n",
      "226  epoch, regularization loss:  0.014569408\n",
      "226  loss to real solution:  0.01582550203685206\n",
      "227  epoch, loss:  -0.092956424\n",
      "227  epoch, regularization loss:  0.0147228185\n",
      "227  loss to real solution:  0.01583278148887239\n",
      "228  epoch, loss:  -0.09293939\n",
      "228  epoch, regularization loss:  0.014877933\n",
      "228  loss to real solution:  0.01584024295929541\n",
      "229  epoch, loss:  -0.09294548\n",
      "229  epoch, regularization loss:  0.0150345005\n",
      "229  loss to real solution:  0.015847609643000856\n",
      "230  epoch, loss:  -0.09295487\n",
      "230  epoch, regularization loss:  0.015192514\n",
      "230  loss to real solution:  0.015854768572896215\n",
      "231  epoch, loss:  -0.09296795\n",
      "231  epoch, regularization loss:  0.0153519595\n",
      "231  loss to real solution:  0.015861715101352432\n",
      "232  epoch, loss:  -0.09301396\n",
      "232  epoch, regularization loss:  0.015512239\n",
      "232  loss to real solution:  0.015867944840832895\n",
      "233  epoch, loss:  -0.09305036\n",
      "233  epoch, regularization loss:  0.015674101\n",
      "233  loss to real solution:  0.015874080429889737\n",
      "234  epoch, loss:  -0.09302336\n",
      "234  epoch, regularization loss:  0.015837513\n",
      "234  loss to real solution:  0.015880083393820587\n",
      "235  epoch, loss:  -0.09303525\n",
      "235  epoch, regularization loss:  0.016002385\n",
      "235  loss to real solution:  0.015885877214444\n",
      "236  epoch, loss:  -0.093050435\n",
      "236  epoch, regularization loss:  0.016168848\n",
      "236  loss to real solution:  0.01589155177786416\n",
      "237  epoch, loss:  -0.093062155\n",
      "237  epoch, regularization loss:  0.016336937\n",
      "237  loss to real solution:  0.01589712668078502\n",
      "238  epoch, loss:  -0.093088515\n",
      "238  epoch, regularization loss:  0.016506944\n",
      "238  loss to real solution:  0.015902906031470557\n",
      "239  epoch, loss:  -0.09306014\n",
      "239  epoch, regularization loss:  0.01667889\n",
      "239  loss to real solution:  0.01590884579722901\n",
      "240  epoch, loss:  -0.09305629\n",
      "240  epoch, regularization loss:  0.016852379\n",
      "240  loss to real solution:  0.015914579869466546\n",
      "241  epoch, loss:  -0.09307272\n",
      "241  epoch, regularization loss:  0.017027486\n",
      "241  loss to real solution:  0.015920154101595585\n",
      "242  epoch, loss:  -0.09308139\n",
      "242  epoch, regularization loss:  0.017204076\n",
      "242  loss to real solution:  0.01592545556102149\n",
      "243  epoch, loss:  -0.09310187\n",
      "243  epoch, regularization loss:  0.0173823\n",
      "243  loss to real solution:  0.015930575667086878\n",
      "244  epoch, loss:  -0.09311919\n",
      "244  epoch, regularization loss:  0.01756215\n",
      "244  loss to real solution:  0.015935526733613065\n",
      "245  epoch, loss:  -0.093096346\n",
      "245  epoch, regularization loss:  0.017743599\n",
      "245  loss to real solution:  0.015940269423451045\n",
      "246  epoch, loss:  -0.09311535\n",
      "246  epoch, regularization loss:  0.017926661\n",
      "246  loss to real solution:  0.01594479990350469\n",
      "247  epoch, loss:  -0.09313658\n",
      "247  epoch, regularization loss:  0.018111415\n",
      "247  loss to real solution:  0.01594919737512273\n",
      "248  epoch, loss:  -0.09314093\n",
      "248  epoch, regularization loss:  0.018298218\n",
      "248  loss to real solution:  0.015953734898490512\n",
      "249  epoch, loss:  -0.09313176\n",
      "249  epoch, regularization loss:  0.0184871\n",
      "249  loss to real solution:  0.01595843920945352\n",
      "250  epoch, loss:  -0.0931749\n",
      "250  epoch, regularization loss:  0.018677602\n",
      "250  loss to real solution:  0.01596293595826125\n",
      "251  epoch, loss:  -0.09316012\n",
      "251  epoch, regularization loss:  0.0188692\n",
      "251  loss to real solution:  0.01596676081323161\n",
      "252  epoch, loss:  -0.0931586\n",
      "252  epoch, regularization loss:  0.019062517\n",
      "252  loss to real solution:  0.01597039885267948\n",
      "253  epoch, loss:  -0.09315423\n",
      "253  epoch, regularization loss:  0.01925754\n",
      "253  loss to real solution:  0.015973830336159806\n",
      "254  epoch, loss:  -0.09317396\n",
      "254  epoch, regularization loss:  0.019454239\n",
      "254  loss to real solution:  0.015977042758196464\n",
      "255  epoch, loss:  -0.09319601\n",
      "255  epoch, regularization loss:  0.019652762\n",
      "255  loss to real solution:  0.01598010707898152\n",
      "256  epoch, loss:  -0.09320276\n",
      "256  epoch, regularization loss:  0.019852966\n",
      "256  loss to real solution:  0.015982934753994434\n",
      "257  epoch, loss:  -0.09320357\n",
      "257  epoch, regularization loss:  0.020055361\n",
      "257  loss to real solution:  0.015985902001620072\n",
      "258  epoch, loss:  -0.09318907\n",
      "258  epoch, regularization loss:  0.020260032\n",
      "258  loss to real solution:  0.015989043271809878\n",
      "259  epoch, loss:  -0.09320686\n",
      "259  epoch, regularization loss:  0.020466322\n",
      "259  loss to real solution:  0.01599186759286368\n",
      "260  epoch, loss:  -0.09323464\n",
      "260  epoch, regularization loss:  0.020674344\n",
      "260  loss to real solution:  0.01599443131129455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261  epoch, loss:  -0.093206234\n",
      "261  epoch, regularization loss:  0.020884184\n",
      "261  loss to real solution:  0.01599677199144452\n",
      "262  epoch, loss:  -0.093190156\n",
      "262  epoch, regularization loss:  0.021095648\n",
      "262  loss to real solution:  0.01599878882288546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c3e8dcc8deb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-1625469fc141>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(mod, initial_lr, milestones, gamma, iterations, mm)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m#and step the optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练the_model\n",
    "train(the_model, initial_lr=0.0001*learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate time for grid method\n",
    "def train(mod, initial_lr=learning_rate, milestones=[400], gamma=0.1, iterations=iterations, mm=1):\n",
    "    optimizer = torch.optim.Adam(mod.parameters(), lr=initial_lr)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "    mm = 1\n",
    "    points = np.arange(-1, 1, 0.1)\n",
    "    xs, ys = np.meshgrid(points, points)\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    xl, yl = xs.size()\n",
    "\n",
    "    #start = time.time()\n",
    "    for k in range(iterations):\n",
    "        n1 = 0\n",
    "        loss = torch.zeros(1)\n",
    "        for i in range(xl):\n",
    "            for j in range(yl):        \n",
    "                x_input = np.zeros(m)\n",
    "                x_input[0] = xs[i, j]\n",
    "                x_input[1] = ys[i, j]\n",
    "                if x_input[0] ** 2 + x_input[1] ** 2 < 1:\n",
    "                    n1 += 1\n",
    "                    x_input = torch.tensor(x_input).float()\n",
    "                    y = mod(x_input)\n",
    "\n",
    "                    x1 = torch.zeros(m)\n",
    "                    x2 = torch.zeros(m)\n",
    "                    x1[0] = 0.0001\n",
    "                    x2[1] = 0.0001\n",
    "                    x_input_1 = x_input.float() + x1\n",
    "                    x_input_2 = x_input.float() + x2\n",
    "                    x_input_3 = x_input.float() - x1\n",
    "                    x_input_4 = x_input.float() - x2\n",
    "                    x_input_grad_1 = (mod(x_input_1) - y) / 0.0001\n",
    "                    x_input_grad_2 = (mod(x_input_2) - y) / 0.0001\n",
    "                    x_input_2_grad_x = (mod(x_input_1) + the_model(x_input_3) - 2 * y) / 0.0001**2\n",
    "                    x_input_2_grad_y = (mod(x_input_2) + the_model(x_input_4) - 2 * y) / 0.0001**2\n",
    "\n",
    "                    loss += 0.5 * ((x_input_grad_1) ** 2 + (x_input_grad_2) ** 2) - y #+ gamma * (x_input_2_grad_x + x_input_2_grad_y) ** 2\n",
    "                    #loss += 0.5 * ((x_input.grad.float()[0]) ** 2 + (x_input.grad.float()[1]) ** 2) + y\n",
    "                    #loss = gamma * (x_input_2_grad_x + x_input_2_grad_y) ** 2\n",
    "        loss /= n1\n",
    "\n",
    "        regularization = torch.zeros(1)\n",
    "        for t in range(n2):\n",
    "            theta = t / n2 * (2 * pi)\n",
    "            x_input = np.zeros(m)\n",
    "            x_input[0] = cos(theta)\n",
    "            x_input[1] = sin(theta)\n",
    "            x_input = torch.tensor(x_input).float()\n",
    "            y = mod(x_input)\n",
    "            regularization += y**2 \n",
    "        regularization *= mm / n2\n",
    "        if gamma < 500:\n",
    "            gamma = gamma * 1.01\n",
    "        if mm < 500:\n",
    "            mm = mm * 1.01\n",
    "\n",
    "        #print loss\n",
    "        print(k, \" epoch, loss: \", loss.data[0].numpy())\n",
    "        print(k, \" epoch, regularization loss: \", regularization.data[0].numpy())\n",
    "        print(k, \" loss to real solution: \", cal_loss(mod))\n",
    "        if cal_loss(the_model) < 0.0001:\n",
    "            break\n",
    "\n",
    "        loss += regularization\n",
    "\n",
    "        #and step the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "    #stop = time.time()\n",
    "    #print(stop - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
